{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdacb42e-e7dc-47b5-96f1-0eee8c9801c0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09c5a734-56c4-4b42-b634-aa3d06f57655",
     "showTitle": false,
     "title": "--i18n-2cf41655-1957-4aa5-a4f0-b9ec55ea213b"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "# MLflow Lab\n",
    "\n",
    "In this lab we will explore the path to moving models to production with MLflow using the following steps:\n",
    "\n",
    "1. Load in Airbnb dataset, and save both training dataset and test dataset as Delta tables\n",
    "1. Train an MLlib linear regression model using all the listing features and tracking parameters, metrics artifacts and Delta table version to MLflow\n",
    "1. Register this initial model and move it to staging using MLflow Model Registry\n",
    "1. Add a new column, **`log_price`** to both our train and test table and update the corresponding Delta tables\n",
    "1. Train a second MLlib linear regression model, this time using **`log_price`** as our target and training on all features, tracking to MLflow \n",
    "1. Compare the performance of the different runs by looking at the underlying data versions for both models\n",
    "1. Move the better performing model to production in MLflow model registry\n",
    "\n",
    "## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Learning Objectives:<br>\n",
    "\n",
    "By the end of this lab, you should be able to;\n",
    "* Create Delta tables from existing data\n",
    "* Explain Delta history feature and history retention policy\n",
    "* Track a model fit process with MLflow\n",
    "* Register a model with MLflow Model Registry\n",
    "* Manage MLflow model lifecycle\n",
    "* Select best model and move it production with MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5585807a-6459-42e0-ac36-4a978fd80258",
     "showTitle": false,
     "title": "--i18n-a5e8f936-8a26-47c1-b388-476c16d4addb"
    }
   },
   "source": [
    "## Lab Setup\n",
    "\n",
    "The first thing we're going to do is to **run setup script**. This script will define the required configuration variables that are scoped to each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ef7a5be-26e6-435a-8960-c0ad333bb93f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nPython interpreter will be restarted.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting the learning environment:\n| No action taken\n\nSkipping install of existing datasets to \"dbfs:/mnt/dbacademy-datasets/scalable-machine-learning-with-apache-spark/v02\"\n\nValidating the locally installed datasets:\n| listing local files...(3 seconds)\n| validation completed...(3 seconds total)\n\nCreating & using the schema \"charlie_ohara_4mi2_da_sml\" in the catalog \"hive_metastore\"...(0 seconds)\n\nPredefined tables in \"charlie_ohara_4mi2_da_sml\":\n| -none-\n\nPredefined paths variables:\n| DA.paths.working_dir: dbfs:/mnt/dbacademy-users/charlie.ohara@standard.ai/scalable-machine-learning-with-apache-spark\n| DA.paths.user_db:     dbfs:/mnt/dbacademy-users/charlie.ohara@standard.ai/scalable-machine-learning-with-apache-spark/database.db\n| DA.paths.datasets:    dbfs:/mnt/dbacademy-datasets/scalable-machine-learning-with-apache-spark/v02\n\nSetup completed (9 seconds)\n"
     ]
    }
   ],
   "source": [
    "%run \"../Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ee1018e-d5c4-4f4f-94a0-277ef67e2813",
     "showTitle": false,
     "title": "--i18n-197ad07c-dead-4444-82de-67353d81dcb0"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "##  Step 1. Creating Delta Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "faa2af03-8031-48c3-b132-41c611c46026",
     "showTitle": false,
     "title": "--i18n-8e9e809d-4142-43d8-b361-830099a02d06"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "Data versioning is an advantage of using Delta Lake, which preserves previous versions of datasets so that you can restore later.\n",
    "\n",
    "Let's split our dataset into train and test datasets, and writing them out in Delta format. You can read more at the Delta Lake <a href=\"https://docs.delta.io/latest/index.html\" target=\"_blank\">documentation</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50d9e258-7265-4d18-8b5a-19f68c2e728a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_path = \"dbfs:/mnt/dbacademy-datasets/scalable-machine-learning-with-apache-spark/v02/airbnb/sf-listings/sf-listings-2019-03-06-clean.delta/\"\n",
    "airbnb_df = spark.read.format(\"delta\").load(file_path)\n",
    "\n",
    "train_df, test_df = airbnb_df.randomSplit([.8, .2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57f75b53-ef5e-41a7-a2a8-600ca368c5f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_delta_path = f\"dbfs:/mnt/dbacademy-users/charlie.ohara@standard.ai/scalable-machine-learning-with-apache-spark/train.delta\"\n",
    "test_delta_path = f\"dbfs:/mnt/dbacademy-users/charlie.ohara@standard.ai/scalable-machine-learning-with-apache-spark/test.delta\"\n",
    "\n",
    "# In case paths already exists\n",
    "dbutils.fs.rm(train_delta_path, True)\n",
    "dbutils.fs.rm(test_delta_path, True)\n",
    "\n",
    "# write our test and train data as a delta table so we can keep version history   \n",
    "train_df.write.mode(\"overwrite\").format(\"delta\").save(train_delta_path)\n",
    "test_df.write.mode(\"overwrite\").format(\"delta\").save(test_delta_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e5500f7-15f1-4ff0-a34f-857904144af9",
     "showTitle": false,
     "title": "--i18n-ead09bc6-c6f2-4dfa-bc9c-ddf41accc8f8"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "Let's now read in our train and test Delta tables, specifying that we want the first version of these tables. This <a href=\"https://databricks.com/blog/2019/02/04/introducing-delta-time-travel-for-large-scale-data-lakes.html\" target=\"_blank\">blog post</a> has a great example of how to read in a Delta table at a given version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "364ac2e9-da98-4f28-9eb8-f43abe39e073",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "data_version = 0\n",
    "train_delta = spark.read.format(\"delta\").option(\"versionAsOf\", data_version).load(train_delta_path)\n",
    "test_delta = spark.read.format(\"delta\").option(\"versionAsOf\", data_version).load(test_delta_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a17fd43-36e3-4fa6-aa14-91f0b55ff67c",
     "showTitle": false,
     "title": "--i18n-2bf375c9-fb36-47a3-b973-82fa805e8b22"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "### Review Delta Table History\n",
    "All the transactions for this table are stored within this table including the initial set of insertions, update, delete, merge, and inserts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24ed45b1-fff4-4add-9d3b-dca08b5cbc9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th><th>engineInfo</th></tr></thead><tbody><tr><td>0</td><td>2024-02-09T16:55:56.235+0000</td><td>6043631322962989</td><td>charlie.ohara@standard.ai</td><td>WRITE</td><td>Map(mode -> Overwrite, partitionBy -> [])</td><td>null</td><td>List(3021276973847115)</td><td>0105-195350-nkhe7nqr</td><td>null</td><td>WriteSerializable</td><td>false</td><td>Map(numFiles -> 4, numOutputRows -> 5786, numOutputBytes -> 209126)</td><td>null</td><td>Databricks-Runtime/12.2.x-cpu-ml-scala2.12</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         0,
         "2024-02-09T16:55:56.235+0000",
         "6043631322962989",
         "charlie.ohara@standard.ai",
         "WRITE",
         {
          "mode": "Overwrite",
          "partitionBy": "[]"
         },
         null,
         [
          "3021276973847115"
         ],
         "0105-195350-nkhe7nqr",
         null,
         "WriteSerializable",
         false,
         {
          "numFiles": "4",
          "numOutputBytes": "209126",
          "numOutputRows": "5786"
         },
         null,
         "Databricks-Runtime/12.2.x-cpu-ml-scala2.12"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "version",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "userId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "userName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operation",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operationParameters",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "job",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"jobId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobRunId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"runId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobOwnerId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"triggerType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "notebook",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"notebookId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "clusterId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "readVersion",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "isolationLevel",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "isBlindAppend",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "operationMetrics",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "userMetadata",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "engineInfo",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(spark.sql(f\"DESCRIBE HISTORY delta.`{train_delta_path}`\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63dd68e5-6c58-47bd-b9d4-dfb538d788af",
     "showTitle": false,
     "title": "--i18n-d3d7251e-c070-4d54-844f-ec6880079e5b"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "By default Delta tables <a href=\"https://docs.databricks.com/delta/delta-batch.html#data-retention\" target=\"_blank\">keep a commit history of 30 days</a>. This retention period can be adjusted by setting **`delta.logRetentionDuration`**, which will determine how far back in time you can go. Note that setting this can result in storage costs to go up. \n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_24.png\"/> Be aware that versioning with Delta in this manner may not be feasible as a long term solution. The retention period of Delta tables can be increased, but with that comes additional costs to storage. Alternative methods of data versioning when training models and tracking to MLflow is to save copies of the datasets, either as an MLflow artifact (for a small dataset), or save to a separate distributed location and record the location of the underlying dataset as a tag in MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72c19a17-987d-4617-bc7a-c9093c68a3f7",
     "showTitle": false,
     "title": "--i18n-ffe159a7-e5dd-49fd-9059-e399237005a7"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "## Step 2. Log Initial Run to MLflow\n",
    "\n",
    "Let's first log a run to MLflow where we use all features. We use the same approach with RFormula as before. This time however, let's also log both the version of our data and the data path to MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35d589eb-a862-4404-bf19-1e007e817871",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/02/09 17:32:49 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n/databricks/python/lib/python3.9/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import RFormula\n",
    "\n",
    "with mlflow.start_run(run_name=\"lr_model\") as run:\n",
    "    # Log parameters\n",
    "    # used for later reference\n",
    "    mlflow.log_param(\"data_path\", train_delta_path)  \n",
    "    # TODO: Log label: price-all-features\n",
    "    mlflow.log_param(\"label\", \"price-all-features\")\n",
    "    # TODO: Log data_version: data_version\n",
    "    mlflow.log_param(\"data_version\", data_version)\n",
    "\n",
    "    # Create pipeline\n",
    "    # This essentially handles mushing all our features into 1 array\n",
    "    r_formula = RFormula(formula=\"price ~ .\", featuresCol=\"features\", labelCol=\"price\", handleInvalid=\"skip\")\n",
    "    # Then passing those features into linear regression algorithm \n",
    "    lr = LinearRegression(labelCol=\"price\", featuresCol=\"features\")\n",
    "    # Combining the data prep step with the actually applying the algorithm step in a pipeline \n",
    "    pipeline = Pipeline(stages = [r_formula, lr])\n",
    "    # Then creating the model\n",
    "    # using the linear regression algorithm to predict the price based on the training data mapping of features to price\n",
    "    model = pipeline.fit(train_delta)\n",
    "\n",
    "    # Log pipeline\n",
    "    # TODO: Log model: model\n",
    "    mlflow.spark.log_model(model, \"model\")\n",
    "\n",
    "    # Create predictions and metrics\n",
    "    # Use transform to apply the model to predict the price of the test data \n",
    "    pred_df = model.transform(test_delta)\n",
    "    # evalute the performance using rmse and r2 \n",
    "    regression_evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\")\n",
    "    rmse = regression_evaluator.setMetricName(\"rmse\").evaluate(pred_df)\n",
    "    r2 = regression_evaluator.setMetricName(\"r2\").evaluate(pred_df)\n",
    "\n",
    "    # Log metrics\n",
    "    # TODO: Log RMSE\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    # TODO: Log R2 - r2 closer to 1 = perfect fit, closer to 0 = terrible fit = terrible at predicting \n",
    "    mlflow.log_metric(\"r2\", r2)\n",
    "\n",
    "    run_id = run.info.run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9320f6ef-3fef-46dd-837d-17fbf512d477",
     "showTitle": false,
     "title": "--i18n-3bac0fef-149d-4c25-9db1-94fbcd63ba13"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "## Step 3. Register Model and Move to Staging Using MLflow Model Registry\n",
    "\n",
    "We are happy with the performance of the above model and want to move it to staging. Let's create the model and register it to the MLflow model registry.\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_24.png\"/> Make sure the path to **`model_uri`** matches the subdirectory (the second argument to **`mlflow.log_model()`**) included above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50e0c563-8b45-4dd8-8b8f-8777ee05986d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: mllib-lr_charlie-ohara-4mi2-da-sml\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'mllib-lr_charlie-ohara-4mi2-da-sml'.\n2024/02/09 17:36:46 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: mllib-lr_charlie-ohara-4mi2-da-sml, version 1\nCreated version '1' of model 'mllib-lr_charlie-ohara-4mi2-da-sml'.\n"
     ]
    }
   ],
   "source": [
    "model_uri = f\"runs:/{run_id}/model\"\n",
    "\n",
    "model_name = \"mllib-lr_charlie-ohara-4mi2-da-sml\"\n",
    "print(f\"Model Name: {model_name}\\n\")\n",
    "\n",
    "# this registers a model in the Machine Learning models tab which then can be used for production \n",
    "model_details = mlflow.register_model(model_uri=model_uri, name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e64d7e8a-24c3-476b-98d9-3daeac661454",
     "showTitle": false,
     "title": "--i18n-78b33d27-0815-4d31-80a0-5e110aa96224"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "Transition model to staging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be005fc6-9b56-4d70-839a-1b0344014379",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[15]: <ModelVersion: creation_timestamp=1707500206031, current_stage='Staging', description='', last_updated_timestamp=1707500260994, name='mllib-lr_charlie-ohara-4mi2-da-sml', run_id='9a48e8310b384809882fd0b8b48e0cd6', run_link='', source='dbfs:/databricks/mlflow-tracking/3021276973847115/9a48e8310b384809882fd0b8b48e0cd6/artifacts/model', status='READY', status_message='', tags={}, user_id='6043631322962989', version='1'>"
     ]
    }
   ],
   "source": [
    "from mlflow.tracking.client import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "client.transition_model_version_stage(\n",
    "    name=model_name,\n",
    "    version=1,\n",
    "    stage=\"Staging\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eca710fb-b2c8-4f0a-b91f-29cc5953096c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define a utility method to wait until the model is ready\n",
    "def wait_for_model(model_name, version, stage=\"None\", status=\"READY\", timeout=300):\n",
    "    import time\n",
    "\n",
    "    last_stage = \"unknown\"\n",
    "    last_status = \"unknown\"\n",
    "\n",
    "    for i in range(timeout): \n",
    "        model_version_details = client.get_model_version(name=model_name, version=version)\n",
    "        last_stage = str(model_version_details.current_stage)\n",
    "        last_status = str(model_version_details.status)\n",
    "        if last_status == str(status) and last_stage == str(stage):\n",
    "            return\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "    raise Exception(f\"The model {model_name} v{version} was not {status} after {timeout} seconds: {last_status}/{last_stage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f59eeaa4-30b8-4b47-851c-1c740b3f2025",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Force our notebook to block until the model is ready\n",
    "wait_for_model(model_name, 1, stage=\"Staging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7204bccb-6bde-4bd9-afb7-caa8091a967c",
     "showTitle": false,
     "title": "--i18n-b5f74e40-1806-46ab-9dd0-97b82d8f297e"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "Add a model description using <a href=\"https://mlflow.org/docs/latest/python_api/mlflow.client.html#mlflow.client.MlflowClient.update_registered_model\" target=\"_blank\">update_registered_model</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66f76364-68fd-4e9a-aad3-71d2a49f0bc7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[18]: <RegisteredModel: creation_timestamp=1707500205685, description='Demo model description', last_updated_timestamp=1707500352145, latest_versions=[], name='mllib-lr_charlie-ohara-4mi2-da-sml', tags={}>"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "client.update_registered_model(model_name, \"Demo model description\") # general description for the model, not the specific version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f0e96a6-f48a-4d88-b2d9-4103ec9816e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "wait_for_model(model_details.name, 1, stage=\"Staging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c4dac78-0286-425c-bd2f-bd55e4299b8e",
     "showTitle": false,
     "title": "--i18n-03dff1c0-5c7b-473f-83ec-4a8283427280"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "##  Step 4. Feature Engineering: Evolve Data Schema\n",
    "\n",
    "We now want to do some feature engineering with the aim of improving model performance; we can use Delta Lake to track older versions of the dataset. \n",
    "\n",
    "We will add **`log_price`** as a new column and update our Delta table with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c80af13e-a95f-4135-8fef-6a7b651013ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, log, exp\n",
    "\n",
    "# Create a new log_price column for both train and test datasets\n",
    "train_new = train_delta.withColumn(\"log_price\", log(col(\"price\")))\n",
    "test_new = test_delta.withColumn(\"log_price\", log(col(\"price\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a39cfaf-a171-4f78-a162-5e350e47e725",
     "showTitle": false,
     "title": "--i18n-565313ed-2bca-4cc6-af87-1c0d509c0a69"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "Save the updated DataFrames to **`train_delta_path`** and **`test_delta_path`**, respectively, passing the **`mergeSchema`** option to safely evolve its schema. \n",
    "\n",
    "Take a look at this <a href=\"https://databricks.com/blog/2019/09/24/diving-into-delta-lake-schema-enforcement-evolution.html\" target=\"_blank\">blog</a> on Delta Lake for more information about **`mergeSchema`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c697460-131b-433e-b2db-daac8555a47d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "train_new.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(train_delta_path) # schema changed because we added a new column\n",
    "test_new.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(test_delta_path) # schema changed because we added a new column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d56bbfaa-7193-4bf6-9d2c-711800c9c093",
     "showTitle": false,
     "title": "--i18n-735a36b6-7510-4e4f-9df8-4ae51f9f87dc"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "Look at the difference between the original & modified schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58bafcb4-79ed-4ea0-a6b1-28c5972332f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[24]: {StructField('log_price', DoubleType(), True)}"
     ]
    }
   ],
   "source": [
    "set(train_new.schema.fields) ^ set(train_delta.schema.fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39baa819-7c2e-426e-8aa2-1240a121965b",
     "showTitle": false,
     "title": "--i18n-0c7c986b-1346-4ff1-a4e2-ee190891a5bf"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "Let's review the Delta history of our **`train_delta`** table and load in the most recent versions of our train and test Delta tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "543b31fe-9f23-4f9c-ab40-e00377d01558",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th><th>engineInfo</th></tr></thead><tbody><tr><td>1</td><td>2024-02-09T17:42:00.198+0000</td><td>6043631322962989</td><td>charlie.ohara@standard.ai</td><td>WRITE</td><td>Map(mode -> Overwrite, partitionBy -> [])</td><td>null</td><td>List(3021276973847115)</td><td>0105-195350-nkhe7nqr</td><td>0</td><td>WriteSerializable</td><td>false</td><td>Map(numFiles -> 4, numOutputRows -> 5786, numOutputBytes -> 226210)</td><td>null</td><td>Databricks-Runtime/12.2.x-cpu-ml-scala2.12</td></tr><tr><td>0</td><td>2024-02-09T16:55:56.235+0000</td><td>6043631322962989</td><td>charlie.ohara@standard.ai</td><td>WRITE</td><td>Map(mode -> Overwrite, partitionBy -> [])</td><td>null</td><td>List(3021276973847115)</td><td>0105-195350-nkhe7nqr</td><td>null</td><td>WriteSerializable</td><td>false</td><td>Map(numFiles -> 4, numOutputRows -> 5786, numOutputBytes -> 209126)</td><td>null</td><td>Databricks-Runtime/12.2.x-cpu-ml-scala2.12</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "2024-02-09T17:42:00.198+0000",
         "6043631322962989",
         "charlie.ohara@standard.ai",
         "WRITE",
         {
          "mode": "Overwrite",
          "partitionBy": "[]"
         },
         null,
         [
          "3021276973847115"
         ],
         "0105-195350-nkhe7nqr",
         0,
         "WriteSerializable",
         false,
         {
          "numFiles": "4",
          "numOutputBytes": "226210",
          "numOutputRows": "5786"
         },
         null,
         "Databricks-Runtime/12.2.x-cpu-ml-scala2.12"
        ],
        [
         0,
         "2024-02-09T16:55:56.235+0000",
         "6043631322962989",
         "charlie.ohara@standard.ai",
         "WRITE",
         {
          "mode": "Overwrite",
          "partitionBy": "[]"
         },
         null,
         [
          "3021276973847115"
         ],
         "0105-195350-nkhe7nqr",
         null,
         "WriteSerializable",
         false,
         {
          "numFiles": "4",
          "numOutputBytes": "209126",
          "numOutputRows": "5786"
         },
         null,
         "Databricks-Runtime/12.2.x-cpu-ml-scala2.12"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "version",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "userId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "userName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operation",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operationParameters",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "job",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"jobId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobRunId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"runId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobOwnerId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"triggerType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "notebook",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"notebookId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "clusterId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "readVersion",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "isolationLevel",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "isBlindAppend",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "operationMetrics",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "userMetadata",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "engineInfo",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(spark.sql(f\"DESCRIBE HISTORY delta.`{train_delta_path}`\")) # interesting that history is kept even with overwrite using delta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2a581ad-0fbb-4a55-8d8d-b631aca09d3f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_version = 1 # read in based on version, assuming latest is the default \n",
    "train_delta_new = spark.read.format(\"delta\").option(\"versionAsOf\", data_version).load(train_delta_path)  \n",
    "test_delta_new = spark.read.format(\"delta\").option(\"versionAsOf\", data_version).load(test_delta_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66183782-1902-45bd-90e4-86b91f470bc9",
     "showTitle": false,
     "title": "--i18n-f29c99ca-b92c-4f74-8bf5-c74070a8cd50"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "## Step 5. Use **`log_price`** as Target and Track Run with MLflow\n",
    "\n",
    "Retrain the model on the updated data and compare its performance to the original, logging results to MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01315f00-0f91-44af-8837-c18a38462d34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/02/09 17:45:48 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\nRegistered model 'mllib-lr_charlie-ohara-4mi2-da-sml' already exists. Creating a new version of this model...\n2024/02/09 17:47:11 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: mllib-lr_charlie-ohara-4mi2-da-sml, version 2\nCreated version '2' of model 'mllib-lr_charlie-ohara-4mi2-da-sml'.\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=\"lr_log_model\") as run:\n",
    "    # Log parameters\n",
    "    # run again using log price instead \n",
    "    mlflow.log_param(\"label\", \"log-price\")\n",
    "    # specify the delta data version used \n",
    "    mlflow.log_param(\"data_version\", data_version)\n",
    "    # specify the source path \n",
    "    mlflow.log_param(\"data_path\", train_delta_path)    \n",
    "\n",
    "    # Create pipeline\n",
    "    # define the feature engineering \n",
    "    r_formula = RFormula(formula=\"log_price ~ . - price\", featuresCol=\"features\", labelCol=\"log_price\", handleInvalid=\"skip\")  \n",
    "    # define the algorithm used \n",
    "    lr = LinearRegression(labelCol=\"log_price\", predictionCol=\"log_prediction\")\n",
    "    # combine the steps needed \n",
    "    pipeline = Pipeline(stages = [r_formula, lr])\n",
    "    # create a model fitting the new data with log price, predicting the log price \n",
    "    pipeline_model = pipeline.fit(train_delta_new)\n",
    "\n",
    "    # Log model and update the registered model\n",
    "    mlflow.spark.log_model(\n",
    "        spark_model=pipeline_model,\n",
    "        artifact_path=\"log-model\",\n",
    "        registered_model_name=model_name\n",
    "    )  \n",
    "\n",
    "    # Create predictions and metrics\n",
    "    pred_df = pipeline_model.transform(test_delta)\n",
    "    exp_df = pred_df.withColumn(\"prediction\", exp(col(\"log_prediction\")))\n",
    "    rmse = regression_evaluator.setMetricName(\"rmse\").evaluate(exp_df)\n",
    "    r2 = regression_evaluator.setMetricName(\"r2\").evaluate(exp_df)\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.log_metric(\"r2\", r2)  \n",
    "\n",
    "    run_id = run.info.run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8921d68f-39f0-4d7d-b62a-63338acdea10",
     "showTitle": false,
     "title": "--i18n-e5bd7bfb-f445-44b5-a272-c6ae2849ac9f"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "## Step 6. Compare Performance Across Runs by Looking at Delta Table Versions \n",
    "\n",
    "Use MLflow's <a href=\"https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.search_runs\" target=\"_blank\">**`mlflow.search_runs`**</a> API to identify runs according to the version of data the run was trained on. Let's compare our runs according to our data versions.\n",
    "\n",
    "Filter based on **`params.data_path`** and **`params.data_version`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c70746cd-2dbf-4813-abb5-29dca37badc5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>status</th>\n",
       "      <th>artifact_uri</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>metrics.rmse</th>\n",
       "      <th>metrics.r2</th>\n",
       "      <th>params.data_path</th>\n",
       "      <th>params.label</th>\n",
       "      <th>...</th>\n",
       "      <th>tags.mlflow.databricks.notebookPath</th>\n",
       "      <th>tags.mlflow.source.name</th>\n",
       "      <th>tags.mlflow.runName</th>\n",
       "      <th>tags.mlflow.databricks.notebookID</th>\n",
       "      <th>tags.mlflow.source.type</th>\n",
       "      <th>tags.mlflow.log-model.history</th>\n",
       "      <th>tags.mlflow.databricks.cluster.info</th>\n",
       "      <th>tags.mlflow.databricks.notebook.commandID</th>\n",
       "      <th>tags.mlflow.databricks.webappURL</th>\n",
       "      <th>tags.mlflow.databricks.cluster.libraries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9a48e8310b384809882fd0b8b48e0cd6</td>\n",
       "      <td>3021276973847115</td>\n",
       "      <td>FINISHED</td>\n",
       "      <td>dbfs:/databricks/mlflow-tracking/3021276973847...</td>\n",
       "      <td>2024-02-09 17:31:30.133000+00:00</td>\n",
       "      <td>2024-02-09 17:34:21.085000+00:00</td>\n",
       "      <td>350.46762</td>\n",
       "      <td>0.119544</td>\n",
       "      <td>dbfs:/mnt/dbacademy-users/charlie.ohara@standa...</td>\n",
       "      <td>price-all-features</td>\n",
       "      <td>...</td>\n",
       "      <td>/Users/charlie.ohara@standard.ai/scalable-mach...</td>\n",
       "      <td>/Users/charlie.ohara@standard.ai/scalable-mach...</td>\n",
       "      <td>lr_model</td>\n",
       "      <td>3021276973847115</td>\n",
       "      <td>NOTEBOOK</td>\n",
       "      <td>[{\"artifact_path\":\"model\",\"flavors\":{\"spark\":{...</td>\n",
       "      <td>{\"cluster_name\":\"charlie\",\"spark_version\":\"12....</td>\n",
       "      <td>4404113587905816260_6297555016341071165_4a05bd...</td>\n",
       "      <td>https://us-central1.gcp.databricks.com</td>\n",
       "      <td>{\"installable\":[],\"redacted\":[]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>97598c3e8b2a435b864e2e014d63392b</td>\n",
       "      <td>3021276973847115</td>\n",
       "      <td>FAILED</td>\n",
       "      <td>dbfs:/databricks/mlflow-tracking/3021276973847...</td>\n",
       "      <td>2024-02-09 17:30:37.254000+00:00</td>\n",
       "      <td>2024-02-09 17:30:50.984000+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dbfs:/mnt/dbacademy-users/charlie.ohara@standa...</td>\n",
       "      <td>price-all-features</td>\n",
       "      <td>...</td>\n",
       "      <td>/Users/charlie.ohara@standard.ai/scalable-mach...</td>\n",
       "      <td>/Users/charlie.ohara@standard.ai/scalable-mach...</td>\n",
       "      <td>lr_model</td>\n",
       "      <td>3021276973847115</td>\n",
       "      <td>NOTEBOOK</td>\n",
       "      <td>None</td>\n",
       "      <td>{\"cluster_name\":\"charlie\",\"spark_version\":\"12....</td>\n",
       "      <td>4404113587905816260_5855800030435895322_06cca6...</td>\n",
       "      <td>https://us-central1.gcp.databricks.com</td>\n",
       "      <td>{\"installable\":[],\"redacted\":[]}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 26 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>run_id</th>\n      <th>experiment_id</th>\n      <th>status</th>\n      <th>artifact_uri</th>\n      <th>start_time</th>\n      <th>end_time</th>\n      <th>metrics.rmse</th>\n      <th>metrics.r2</th>\n      <th>params.data_path</th>\n      <th>params.label</th>\n      <th>...</th>\n      <th>tags.mlflow.databricks.notebookPath</th>\n      <th>tags.mlflow.source.name</th>\n      <th>tags.mlflow.runName</th>\n      <th>tags.mlflow.databricks.notebookID</th>\n      <th>tags.mlflow.source.type</th>\n      <th>tags.mlflow.log-model.history</th>\n      <th>tags.mlflow.databricks.cluster.info</th>\n      <th>tags.mlflow.databricks.notebook.commandID</th>\n      <th>tags.mlflow.databricks.webappURL</th>\n      <th>tags.mlflow.databricks.cluster.libraries</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9a48e8310b384809882fd0b8b48e0cd6</td>\n      <td>3021276973847115</td>\n      <td>FINISHED</td>\n      <td>dbfs:/databricks/mlflow-tracking/3021276973847...</td>\n      <td>2024-02-09 17:31:30.133000+00:00</td>\n      <td>2024-02-09 17:34:21.085000+00:00</td>\n      <td>350.46762</td>\n      <td>0.119544</td>\n      <td>dbfs:/mnt/dbacademy-users/charlie.ohara@standa...</td>\n      <td>price-all-features</td>\n      <td>...</td>\n      <td>/Users/charlie.ohara@standard.ai/scalable-mach...</td>\n      <td>/Users/charlie.ohara@standard.ai/scalable-mach...</td>\n      <td>lr_model</td>\n      <td>3021276973847115</td>\n      <td>NOTEBOOK</td>\n      <td>[{\"artifact_path\":\"model\",\"flavors\":{\"spark\":{...</td>\n      <td>{\"cluster_name\":\"charlie\",\"spark_version\":\"12....</td>\n      <td>4404113587905816260_6297555016341071165_4a05bd...</td>\n      <td>https://us-central1.gcp.databricks.com</td>\n      <td>{\"installable\":[],\"redacted\":[]}</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>97598c3e8b2a435b864e2e014d63392b</td>\n      <td>3021276973847115</td>\n      <td>FAILED</td>\n      <td>dbfs:/databricks/mlflow-tracking/3021276973847...</td>\n      <td>2024-02-09 17:30:37.254000+00:00</td>\n      <td>2024-02-09 17:30:50.984000+00:00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>dbfs:/mnt/dbacademy-users/charlie.ohara@standa...</td>\n      <td>price-all-features</td>\n      <td>...</td>\n      <td>/Users/charlie.ohara@standard.ai/scalable-mach...</td>\n      <td>/Users/charlie.ohara@standard.ai/scalable-mach...</td>\n      <td>lr_model</td>\n      <td>3021276973847115</td>\n      <td>NOTEBOOK</td>\n      <td>None</td>\n      <td>{\"cluster_name\":\"charlie\",\"spark_version\":\"12....</td>\n      <td>4404113587905816260_5855800030435895322_06cca6...</td>\n      <td>https://us-central1.gcp.databricks.com</td>\n      <td>{\"installable\":[],\"redacted\":[]}</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows × 26 columns</p>\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO\n",
    "data_version = 0\n",
    "\n",
    "mlflow.search_runs(filter_string=f\"params.data_version = '{data_version}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35c9dc48-868b-4f5a-a2da-6a7fcdb36105",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>status</th>\n",
       "      <th>artifact_uri</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>metrics.rmse</th>\n",
       "      <th>metrics.r2</th>\n",
       "      <th>params.data_path</th>\n",
       "      <th>params.label</th>\n",
       "      <th>...</th>\n",
       "      <th>tags.mlflow.databricks.notebookPath</th>\n",
       "      <th>tags.mlflow.source.name</th>\n",
       "      <th>tags.mlflow.runName</th>\n",
       "      <th>tags.mlflow.databricks.notebookID</th>\n",
       "      <th>tags.mlflow.source.type</th>\n",
       "      <th>tags.mlflow.log-model.history</th>\n",
       "      <th>tags.mlflow.databricks.cluster.info</th>\n",
       "      <th>tags.mlflow.databricks.notebook.commandID</th>\n",
       "      <th>tags.mlflow.databricks.webappURL</th>\n",
       "      <th>tags.mlflow.databricks.cluster.libraries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5989338c807248b988596efed30605bc</td>\n",
       "      <td>3021276973847115</td>\n",
       "      <td>FINISHED</td>\n",
       "      <td>dbfs:/databricks/mlflow-tracking/3021276973847...</td>\n",
       "      <td>2024-02-09 17:44:34.614000+00:00</td>\n",
       "      <td>2024-02-09 17:47:54.258000+00:00</td>\n",
       "      <td>355.003697</td>\n",
       "      <td>0.096606</td>\n",
       "      <td>dbfs:/mnt/dbacademy-users/charlie.ohara@standa...</td>\n",
       "      <td>log-price</td>\n",
       "      <td>...</td>\n",
       "      <td>/Users/charlie.ohara@standard.ai/scalable-mach...</td>\n",
       "      <td>/Users/charlie.ohara@standard.ai/scalable-mach...</td>\n",
       "      <td>lr_log_model</td>\n",
       "      <td>3021276973847115</td>\n",
       "      <td>NOTEBOOK</td>\n",
       "      <td>[{\"artifact_path\":\"log-model\",\"flavors\":{\"spar...</td>\n",
       "      <td>{\"cluster_name\":\"charlie\",\"spark_version\":\"12....</td>\n",
       "      <td>4404113587905816260_8057573663244688583_4c76f0...</td>\n",
       "      <td>https://us-central1.gcp.databricks.com</td>\n",
       "      <td>{\"installable\":[],\"redacted\":[]}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 26 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>run_id</th>\n      <th>experiment_id</th>\n      <th>status</th>\n      <th>artifact_uri</th>\n      <th>start_time</th>\n      <th>end_time</th>\n      <th>metrics.rmse</th>\n      <th>metrics.r2</th>\n      <th>params.data_path</th>\n      <th>params.label</th>\n      <th>...</th>\n      <th>tags.mlflow.databricks.notebookPath</th>\n      <th>tags.mlflow.source.name</th>\n      <th>tags.mlflow.runName</th>\n      <th>tags.mlflow.databricks.notebookID</th>\n      <th>tags.mlflow.source.type</th>\n      <th>tags.mlflow.log-model.history</th>\n      <th>tags.mlflow.databricks.cluster.info</th>\n      <th>tags.mlflow.databricks.notebook.commandID</th>\n      <th>tags.mlflow.databricks.webappURL</th>\n      <th>tags.mlflow.databricks.cluster.libraries</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5989338c807248b988596efed30605bc</td>\n      <td>3021276973847115</td>\n      <td>FINISHED</td>\n      <td>dbfs:/databricks/mlflow-tracking/3021276973847...</td>\n      <td>2024-02-09 17:44:34.614000+00:00</td>\n      <td>2024-02-09 17:47:54.258000+00:00</td>\n      <td>355.003697</td>\n      <td>0.096606</td>\n      <td>dbfs:/mnt/dbacademy-users/charlie.ohara@standa...</td>\n      <td>log-price</td>\n      <td>...</td>\n      <td>/Users/charlie.ohara@standard.ai/scalable-mach...</td>\n      <td>/Users/charlie.ohara@standard.ai/scalable-mach...</td>\n      <td>lr_log_model</td>\n      <td>3021276973847115</td>\n      <td>NOTEBOOK</td>\n      <td>[{\"artifact_path\":\"log-model\",\"flavors\":{\"spar...</td>\n      <td>{\"cluster_name\":\"charlie\",\"spark_version\":\"12....</td>\n      <td>4404113587905816260_8057573663244688583_4c76f0...</td>\n      <td>https://us-central1.gcp.databricks.com</td>\n      <td>{\"installable\":[],\"redacted\":[]}</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows × 26 columns</p>\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO\n",
    "data_version = 1\n",
    "\n",
    "mlflow.search_runs(filter_string=f\"params.data_version = '{data_version}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1965961c-1b2e-4802-9cfe-2f1435fc8ba2",
     "showTitle": false,
     "title": "--i18n-fd0fc3ae-7c2e-4d7d-90da-0b6e6b830496"
    }
   },
   "source": [
    "\n",
    "**Question:** Which version of the data produced the best model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33383bde-1dd4-4d4d-bb66-5fed7c9315d9",
     "showTitle": false,
     "title": "--i18n-3056bfcc-7623-4410-8b1b-82cba24ae3dd"
    }
   },
   "source": [
    "\n",
    "## Step 7. Move the Best Performing Model to Production Using MLflow Model Registry\n",
    "\n",
    "Get the most recent model version and move it to production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a27b2d8-fddd-4e52-8d85-3ce089a2cb20",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_version_infos = client.search_model_versions(f\"name = '{model_name}'\")\n",
    "new_model_version = max([model_version_info.version for model_version_info in model_version_infos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c6cb04b-d503-4325-9f5f-0fbe3f153c5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[31]: <ModelVersion: creation_timestamp=1707500831187, current_stage='None', description=('This model version was built using a MLlib Linear Regression model with all '\n 'features and log_price as predictor.'), last_updated_timestamp=1707501163578, name='mllib-lr_charlie-ohara-4mi2-da-sml', run_id='5989338c807248b988596efed30605bc', run_link='', source='dbfs:/databricks/mlflow-tracking/3021276973847115/5989338c807248b988596efed30605bc/artifacts/log-model', status='READY', status_message='', tags={}, user_id='6043631322962989', version='2'>"
     ]
    }
   ],
   "source": [
    "client.update_model_version(\n",
    "    name=model_name,\n",
    "    version=new_model_version,\n",
    "    description=\"This model version was built using a MLlib Linear Regression model with all features and log_price as predictor.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "654875e3-8795-4816-8814-d74fcf5cf82d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[32]: 'READY'"
     ]
    }
   ],
   "source": [
    "model_version_details = client.get_model_version(name=model_name, version=new_model_version)\n",
    "model_version_details.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cb5d0c4-ebb8-4f3c-933b-26d9001b4f31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "wait_for_model(model_name, new_model_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f84fff36-6229-4d3e-a3c9-fe2aa1ea6452",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[35]: <ModelVersion: creation_timestamp=1707500831187, current_stage='Production', description=('This model version was built using a MLlib Linear Regression model with all '\n 'features and log_price as predictor.'), last_updated_timestamp=1707501248525, name='mllib-lr_charlie-ohara-4mi2-da-sml', run_id='5989338c807248b988596efed30605bc', run_link='', source='dbfs:/databricks/mlflow-tracking/3021276973847115/5989338c807248b988596efed30605bc/artifacts/log-model', status='READY', status_message='', tags={}, user_id='6043631322962989', version='2'>"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "# Move Model into Production\n",
    "client.transition_model_version_stage(name=model_name, version=new_model_version, stage=\"production\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78f14758-abc1-43e1-ba3a-8c4c66f20b59",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "wait_for_model(model_name, new_model_version, \"Production\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96dc7ba1-a287-4427-8842-1249e771c470",
     "showTitle": false,
     "title": "--i18n-102094e8-1aa0-4448-9cc4-5e5e36fb5426"
    }
   },
   "source": [
    "\n",
    "\n",
    " \n",
    "\n",
    "Have a look at the MLflow model registry UI to check that your models have been successfully registered. You should see that version 1 of your model is now in staging, with version 2 in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0816e9c-a8a3-4a5c-978a-2331f75db267",
     "showTitle": false,
     "title": "--i18n-f74c46fc-b825-4d73-b41f-c45e6cd360fb"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "To finish the lab, let's clean up by archiving both model versions and deleting the whole model from the registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c558500-621d-4e6c-9a93-149a595168d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[36]: <ModelVersion: creation_timestamp=1707500206031, current_stage='Archived', description='', last_updated_timestamp=1707501265930, name='mllib-lr_charlie-ohara-4mi2-da-sml', run_id='9a48e8310b384809882fd0b8b48e0cd6', run_link='', source='dbfs:/databricks/mlflow-tracking/3021276973847115/9a48e8310b384809882fd0b8b48e0cd6/artifacts/model', status='READY', status_message='', tags={}, user_id='6043631322962989', version='1'>"
     ]
    }
   ],
   "source": [
    "client.transition_model_version_stage(\n",
    "    name=model_name,\n",
    "    version=1,\n",
    "    stage=\"Archived\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "042d88bb-5e17-4168-af16-25527f8dc95b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "wait_for_model(model_name, 1, \"Archived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "783206eb-ede5-4ab6-96eb-61ea9af9af92",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[38]: <ModelVersion: creation_timestamp=1707500831187, current_stage='Archived', description=('This model version was built using a MLlib Linear Regression model with all '\n 'features and log_price as predictor.'), last_updated_timestamp=1707501278413, name='mllib-lr_charlie-ohara-4mi2-da-sml', run_id='5989338c807248b988596efed30605bc', run_link='', source='dbfs:/databricks/mlflow-tracking/3021276973847115/5989338c807248b988596efed30605bc/artifacts/log-model', status='READY', status_message='', tags={}, user_id='6043631322962989', version='2'>"
     ]
    }
   ],
   "source": [
    "client.transition_model_version_stage(\n",
    "    name=model_name,\n",
    "    version=2,\n",
    "    stage=\"Archived\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb66f79f-8f83-4f71-ac2c-c9473d05f0d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "wait_for_model(model_name, 2, \"Archived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85bca7fe-8ed7-4bca-9104-cb14bc70f85a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "client.delete_registered_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "246b85ec-ca4f-423f-b611-a1bdd67e5de9",
     "showTitle": false,
     "title": "--i18n-a2c7fb12-fd0b-493f-be4f-793d0a61695b"
    }
   },
   "source": [
    "\n",
    "## Classroom Cleanup\n",
    "\n",
    "Run the following cell to remove lessons-specific assets created during this lesson:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a35a6cea-e65e-4e8d-96ea-f5467fc65e65",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting the learning environment:\n| dropping the schema \"charlie_ohara_4mi2_da_sml\"...(1 seconds)\n| removing the working directory \"dbfs:/mnt/dbacademy-users/charlie.ohara@standard.ai/scalable-machine-learning-with-apache-spark\"...(0 seconds)\n\nValidating the locally installed datasets:\n| listing local files...(3 seconds)\n| validation completed...(3 seconds total)\n"
     ]
    }
   ],
   "source": [
    "DA.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "916d6522-9e1e-44f6-9dd2-d4d9da2f4ec3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "&copy; 2023 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "ml_05_mlflow_lab",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
