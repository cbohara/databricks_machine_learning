{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a04b9e64-e965-414a-99eb-47f8df68b8c7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37f5db05-8c4a-4e89-be29-cc913729b07c",
     "showTitle": false,
     "title": "--i18n-2ab084da-06ed-457d-834a-1d19353e5c59"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "# Random Forests and Hyperparameter Tuning\n",
    "\n",
    "Now let's take a look at how to tune random forests using grid search and cross validation in order to find the optimal hyperparameters.  Using the Databricks Runtime for ML, MLflow automatically logs metrics of all your experiments with the SparkML cross-validator as well as the best fit model!\n",
    "\n",
    "## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Lesson Objectives:<br>\n",
    "\n",
    "By the end of this lesson, you should be able to;\n",
    "\n",
    "* Tune hyperparameters using Spark MLâ€™s grid search feature\n",
    "* Explain cross validation concepts and how to use cross validation in Spark ML pipelines\n",
    "* Optimize a Spark ML pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffd3428c-e295-409b-9d43-c6221342602a",
     "showTitle": false,
     "title": "--i18n-1e2c921e-1125-4df3-b914-d74bf7a73ab5"
    }
   },
   "source": [
    "## ðŸ“Œ Requirements\n",
    "\n",
    "**Required Databricks Runtime Version:** \n",
    "* Please note that in order to run this notebook, you must use one of the following Databricks Runtime(s): **12.2.x-cpu-ml-scala2.12**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "803433fd-ec5d-4879-837d-c967d3a29b1a",
     "showTitle": false,
     "title": "--i18n-6a1bb996-7b50-4f03-9bcd-3d3ec3069a6d"
    }
   },
   "source": [
    "## Lesson Setup\n",
    "\n",
    "The first thing we're going to do is to **run setup script**. This script will define the required configuration variables that are scoped to each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "779ad2ad-d5a2-4fc3-83b8-056fd9b5a8b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nPython interpreter will be restarted.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting the learning environment:\n| No action taken\n\nSkipping install of existing datasets to \"dbfs:/mnt/dbacademy-datasets/scalable-machine-learning-with-apache-spark/v02\"\n\nValidating the locally installed datasets:\n| listing local files...(3 seconds)\n| validation completed...(3 seconds total)\n\nCreating & using the schema \"charlie_ohara_4mi2_da_sml\" in the catalog \"hive_metastore\"...(1 seconds)\n\nPredefined tables in \"charlie_ohara_4mi2_da_sml\":\n| -none-\n\nPredefined paths variables:\n| DA.paths.working_dir: dbfs:/mnt/dbacademy-users/charlie.ohara@standard.ai/scalable-machine-learning-with-apache-spark\n| DA.paths.user_db:     dbfs:/mnt/dbacademy-users/charlie.ohara@standard.ai/scalable-machine-learning-with-apache-spark/database.db\n| DA.paths.datasets:    dbfs:/mnt/dbacademy-datasets/scalable-machine-learning-with-apache-spark/v02\n\nSetup completed (12 seconds)\n"
     ]
    }
   ],
   "source": [
    "%run \"./Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3f4b2b0-2db4-42ff-af74-53cf3fe42ad0",
     "showTitle": false,
     "title": "--i18n-67393595-40fc-4274-b9ed-40f8ef4f7db1"
    }
   },
   "source": [
    "\n",
    "## Build a Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "499009ba-032e-4f49-905d-21a0e69e088f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# load data and split between train and test data \n",
    "file_path = f\"dbfs:/mnt/dbacademy-datasets/scalable-machine-learning-with-apache-spark/v02/airbnb/sf-listings/sf-listings-2019-03-06-clean.delta/\"\n",
    "airbnb_df = spark.read.format(\"delta\").load(file_path)\n",
    "train_df, test_df = airbnb_df.randomSplit([.8, .2], seed=42)\n",
    "\n",
    "# then do \"feature eng\" \n",
    "categorical_cols = [field for (field, dataType) in train_df.dtypes if dataType == \"string\"]\n",
    "index_output_cols = [x + \"Index\" for x in categorical_cols]\n",
    "\n",
    "string_indexer = StringIndexer(inputCols=categorical_cols, outputCols=index_output_cols, handleInvalid=\"skip\")\n",
    "# vector assemble = ML friendly format adding all numeric and caterogical variables into a single column as a list \n",
    "numeric_cols = [field for (field, dataType) in train_df.dtypes if ((dataType == \"double\") & (field != \"price\"))]\n",
    "assembler_inputs = index_output_cols + numeric_cols\n",
    "vec_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
    "\n",
    "# choose the algo, predicting the price and setting max bins so data can be split into multiple workers evenly\n",
    "rf = RandomForestRegressor(labelCol=\"price\", maxBins=40)\n",
    "stages = [string_indexer, vec_assembler, rf]\n",
    "# run all the steps sequentially \n",
    "pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f86143ed-8050-4518-956e-49ba51b3f7f0",
     "showTitle": false,
     "title": "--i18n-4561938e-90b5-413c-9e25-ef15ba40e99c"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "## ParamGrid\n",
    "\n",
    "First let's take a look at the various hyperparameters we could tune for random forest.\n",
    "\n",
    "**Pop quiz:** what's the difference between a parameter and a hyperparameter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8612fdd-3d51-4057-8e5a-98782bb2c690",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bootstrap: Whether bootstrap samples are used when building trees. (default: True)\ncacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval. (default: False)\ncheckpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\nfeatureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: 'auto' (choose automatically for task: If numTrees == 1, set to 'all'. If numTrees > 1 (forest), set to 'sqrt' for classification and to 'onethird' for regression), 'all' (use all features), 'onethird' (use 1/3 of the features), 'sqrt' (use sqrt(number of features)), 'log2' (use log2(number of features)), 'n' (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = 'auto' (default: auto)\nfeaturesCol: features column name. (default: features)\nimpurity: Criterion used for information gain calculation (case-insensitive). Supported options: variance (default: variance)\nlabelCol: label column name. (default: label, current: price)\nleafCol: Leaf indices column name. Predicted leaf index of each instance in each tree by preorder. (default: )\nmaxBins: Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature. (default: 32, current: 40)\nmaxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30]. (default: 5)\nmaxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. (default: 256)\nminInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\nminInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1)\nminWeightFractionPerNode: Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5). (default: 0.0)\nnumTrees: Number of trees to train (>= 1). (default: 20)\npredictionCol: prediction column name. (default: prediction)\nseed: random seed. (default: 2502083311556356884)\nsubsamplingRate: Fraction of the training data used for learning each decision tree, in range (0, 1]. (default: 1.0)\nweightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "print(rf.explainParams())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef7b2796-490f-4d09-a9d1-69fccb636b2f",
     "showTitle": false,
     "title": "--i18n-819de6f9-75d2-45df-beb1-6b59ecd2cfd2"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "There are a lot of hyperparameters we could tune, and it would take a long time to manually configure.\n",
    "\n",
    "Instead of a manual (ad-hoc) approach, let's use Spark's <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.ParamGridBuilder.html?highlight=paramgridbuilder#pyspark.ml.tuning.ParamGridBuilder\" target=\"_blank\">ParamGridBuilder</a> to find the optimal hyperparameters in a more systematic approach.\n",
    "\n",
    "Let's define a grid of hyperparameters to test:\n",
    "  - **`maxDepth`**: max depth of each decision tree (Use the values **`2, 5`**)\n",
    "  - **`numTrees`**: number of decision trees to train (Use the values **`5, 10`**)\n",
    "\n",
    "**`addGrid()`** accepts the name of the parameter (e.g. **`rf.maxDepth`**), and a list of the possible values (e.g. **`[2, 5]`**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14c0a025-fa8d-4a22-b9ce-a8f76cd231ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "param_grid = (ParamGridBuilder()\n",
    "              .addGrid(rf.maxDepth, [2, 5]) # depth of each tree - list of possible values are depth or 2 or 5\n",
    "              .addGrid(rf.numTrees, [5, 10]) # list of possible values are 5 trees and 10 trees\n",
    "              .build())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c5969b2-8b2e-4353-bfb3-24631f2e6baf",
     "showTitle": false,
     "title": "--i18n-9f043287-11b8-482d-8501-2f7d8b1458ea"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "## Cross Validation\n",
    "\n",
    "We are also going to use 3-fold cross validation to identify the optimal hyperparameters.\n",
    "\n",
    "![crossValidation](https://files.training.databricks.com/images/301/CrossValidation.png)\n",
    "\n",
    "With 3-fold cross-validation, we train on 2/3 of the data, and evaluate with the remaining (held-out) 1/3. We repeat this process 3 times, so each fold gets the chance to act as the validation set. We then average the results of the three rounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26460181-02cb-4eaa-99bc-2eb959bad63c",
     "showTitle": false,
     "title": "--i18n-ec0440ab-071d-4201-be86-5eeedaf80a4f"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "We pass in the **`estimator`** (pipeline), **`evaluator`**, and **`estimatorParamMaps`** to <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.CrossValidator.html?highlight=crossvalidator#pyspark.ml.tuning.CrossValidator\" target=\"_blank\">CrossValidator</a> so that it knows:\n",
    "- Which model to use\n",
    "- How to evaluate the model\n",
    "- What hyperparameters to set for the model\n",
    "\n",
    "We can also set the number of folds we want to split our data into (3), as well as setting a seed so we all have the same split in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dfd015b-68f0-49a9-9254-50ae3ba59cf2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\")\n",
    "\n",
    "cv = CrossValidator(estimator=pipeline, # includes preprocessing of the data and algo we want to use - need to rebuild over and over\n",
    "                    evaluator=evaluator, # defines how we evaluate model success \n",
    "                    estimatorParamMaps=param_grid, # what hyperparameters to use \n",
    "                    numFolds=3, # split our data into 3 - use 2/3 for training and 1/3 for validation, switching up which third is used for validation \n",
    "                    seed=42 # random number to make result reproducable \n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a581d9f-b06a-46de-b1a5-cc258a7d83c4",
     "showTitle": false,
     "title": "--i18n-673c9261-a861-4ace-b008-c04565230a8e"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "**Question**: How many models are we training right now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32807eea-861e-4286-b497-b1e3b2f68b43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_model = cv.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f842c218-9a20-48b0-b3c0-a8a7d7c415d1",
     "showTitle": false,
     "title": "--i18n-2d00b40f-c5e7-4089-890b-a50ccced34c6"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "**Question**: Should we put the pipeline in the cross validator, or the cross validator in the pipeline?\n",
    "\n",
    "It depends if there are estimators or transformers in the pipeline. If you have things like StringIndexer (an estimator) in the pipeline, then you have to refit it every time if you put the entire pipeline in the cross validator.\n",
    "\n",
    "However, if there is any concern about data leakage from the earlier steps, the safest thing is to put the pipeline inside the CV, not the other way. CV first splits the data and then .fit() the pipeline. If it is placed at the end of the pipeline, we potentially can leak the info from hold-out set to train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c399bca6-0304-48a5-894e-1c1c963feac4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv = CrossValidator(estimator=rf, evaluator=evaluator, estimatorParamMaps=param_grid, \n",
    "                    numFolds=3, seed=42)\n",
    "\n",
    "stages_with_cv = [string_indexer, vec_assembler, cv] # much faster to avoid rebuilding the pipeline each time = duplicate work for each pass\n",
    "pipeline = Pipeline(stages=stages_with_cv)\n",
    "\n",
    "pipeline_model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9f097e8-573c-4690-a7e4-4069fecacc80",
     "showTitle": false,
     "title": "--i18n-42652818-f185-45d2-8e96-8204292fea5b"
    }
   },
   "source": [
    "In the current method, only **one MLflow run is logged**, whereas in the previous method, **five runs** were logged. This is because, in the first method, the pipeline was placed within the CrossValidator, which automatically logs all runs. However, in the second method, since **the pipeline only returns the best model without evaluating metrics**, only that single model is seen. Additionally, no evaluation metrics are logged. Essentially, the Pipeline logs all stages and directly returns the best model without performing model evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1445948b-9963-4c19-9ad7-75faf2fac640",
     "showTitle": false,
     "title": "--i18n-dede990c-2551-4c07-8aad-d697ae827e71"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "Let's take a look at the model with the best hyperparameter configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5eb07026-3d56-4f98-86f1-d7171f483ae8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({Param(parent='RandomForestRegressor_7336550d560e', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 2, Param(parent='RandomForestRegressor_7336550d560e', name='numTrees', doc='Number of trees to train (>= 1).'): 5}, 280.13676707305024), ({Param(parent='RandomForestRegressor_7336550d560e', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 2, Param(parent='RandomForestRegressor_7336550d560e', name='numTrees', doc='Number of trees to train (>= 1).'): 10}, 280.00286295955465), ({Param(parent='RandomForestRegressor_7336550d560e', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 5, Param(parent='RandomForestRegressor_7336550d560e', name='numTrees', doc='Number of trees to train (>= 1).'): 5}, 287.9329709115638), ({Param(parent='RandomForestRegressor_7336550d560e', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 5, Param(parent='RandomForestRegressor_7336550d560e', name='numTrees', doc='Number of trees to train (>= 1).'): 10}, 294.6576018771978)]\n4\n"
     ]
    }
   ],
   "source": [
    "results = list(zip(cv_model.getEstimatorParamMaps(), cv_model.avgMetrics))\n",
    "print(results) \n",
    "print(len(results)) # 4\n",
    "# so we do a combo of  the different params = 2 x 2 = 4 models created\n",
    "# model 1 = train 5 trees with depth of 2 \n",
    "# model 2 = train 10 trees with depth of 2\n",
    "# model 3 = train 5 trees with depth of 5\n",
    "# model 4 = train 10 trees with depth of 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3a8637c-c412-4899-82bf-a96e31625791",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE is 0.35841179228176645\nR2 is 0.35841179228176645\n"
     ]
    }
   ],
   "source": [
    "# the pipeline_model will only store the model that had the best performance based on the performance metric we defined in the evaluator \n",
    "pred_df = pipeline_model.transform(test_df)\n",
    "\n",
    "rmse = evaluator.evaluate(pred_df)\n",
    "r2 = evaluator.setMetricName(\"r2\").evaluate(pred_df)\n",
    "print(f\"RMSE is {rmse}\")\n",
    "print(f\"R2 is {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82b02176-06dd-4eff-accf-40cf281363e0",
     "showTitle": false,
     "title": "--i18n-8f80daf2-8f0b-4cab-a8e6-4060c78d94b0"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "Progress!  Looks like we're out-performing decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "babf4ae9-2fc1-41fa-8fe5-7c4be1ee8d95",
     "showTitle": false,
     "title": "--i18n-a2c7fb12-fd0b-493f-be4f-793d0a61695b"
    }
   },
   "source": [
    "\n",
    "## Classroom Cleanup\n",
    "\n",
    "Run the following cell to remove lessons-specific assets created during this lesson:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ee2e9c3-415c-4d56-a4eb-88abe0ae361b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting the learning environment:\n| dropping the schema \"charlie_ohara_4mi2_da_sml\"...(1 seconds)\n| removing the working directory \"dbfs:/mnt/dbacademy-users/charlie.ohara@standard.ai/scalable-machine-learning-with-apache-spark\"...(0 seconds)\n\nValidating the locally installed datasets:\n| listing local files...(3 seconds)\n| validation completed...(3 seconds total)\n"
     ]
    }
   ],
   "source": [
    "DA.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4737bbc-d5f0-451e-9b7f-3c5cf4588918",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "&copy; 2023 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "ml_07_random_forests",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
