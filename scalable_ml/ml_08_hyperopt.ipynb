{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98cb1ea5-e46f-4b94-868e-a962858db8a1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "388c5ea1-2bff-4889-b6f1-78b77ec1d772",
     "showTitle": false,
     "title": "--i18n-1fa7a9c8-3dad-454e-b7ac-555020a4bda8"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "# Hyperopt\n",
    "\n",
    "Hyperopt is a Python library for \"serial and parallel optimization over awkward search spaces, which may include real-valued, discrete, and conditional dimensions\".\n",
    "\n",
    "In the machine learning workflow, hyperopt can be used to distribute/parallelize the hyperparameter optimization process with more advanced optimization strategies than are available in other libraries.\n",
    "\n",
    "\n",
    "## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Learning Objectives:<br>\n",
    "\n",
    "By the end of this lesson, you should be able to;\n",
    "\n",
    "* Define main components of hyperopt for distributed hyperparameter tuning\n",
    "* Utilize hyperopt to find the optimal parameters for a Spark ML model\n",
    "* Compare and contrast common hyperparameter tuning methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21cfd068-7f8f-4f37-ab7b-832fd4f5b1ce",
     "showTitle": false,
     "title": "--i18n-1e2c921e-1125-4df3-b914-d74bf7a73ab5"
    }
   },
   "source": [
    "## ðŸ“Œ Requirements\n",
    "\n",
    "**Required Databricks Runtime Version:** \n",
    "* Please note that in order to run this notebook, you must use one of the following Databricks Runtime(s): **12.2.x-cpu-ml-scala2.12**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a26d270-52bf-4a22-ae5e-731888ea4dfe",
     "showTitle": false,
     "title": "--i18n-6a1bb996-7b50-4f03-9bcd-3d3ec3069a6d"
    }
   },
   "source": [
    "## Lesson Setup\n",
    "\n",
    "The first thing we're going to do is to **run setup script**. This script will define the required configuration variables that are scoped to each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02415368-32af-41c3-bd9f-71b68d5be4bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nPython interpreter will be restarted.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting the learning environment:\n| dropping the schema \"charlie_ohara_4mi2_da_sml\"...(1 seconds)\n| removing the working directory \"dbfs:/mnt/dbacademy-users/charlie.ohara@standard.ai/scalable-machine-learning-with-apache-spark\"...(1 seconds)\n\nSkipping install of existing datasets to \"dbfs:/mnt/dbacademy-datasets/scalable-machine-learning-with-apache-spark/v02\"\n\nValidating the locally installed datasets:\n| listing local files...(3 seconds)\n| validation completed...(3 seconds total)\n\nCreating & using the schema \"charlie_ohara_4mi2_da_sml\" in the catalog \"hive_metastore\"...(1 seconds)\n\nPredefined tables in \"charlie_ohara_4mi2_da_sml\":\n| -none-\n\nPredefined paths variables:\n| DA.paths.working_dir: dbfs:/mnt/dbacademy-users/charlie.ohara@standard.ai/scalable-machine-learning-with-apache-spark\n| DA.paths.user_db:     dbfs:/mnt/dbacademy-users/charlie.ohara@standard.ai/scalable-machine-learning-with-apache-spark/database.db\n| DA.paths.datasets:    dbfs:/mnt/dbacademy-datasets/scalable-machine-learning-with-apache-spark/v02\n\nSetup completed (19 seconds)\n"
     ]
    }
   ],
   "source": [
    "%run \"./Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0cf7104-b2d0-4bf3-82ae-e56cb44e850f",
     "showTitle": false,
     "title": "--i18n-cb6f7ada-c1de-48ef-8d74-59ade1ccaa63"
    }
   },
   "source": [
    "\n",
    "## Introduction to Hyperopt?\n",
    "\n",
    "In the machine learning workflow, hyperopt can be used to distribute/parallelize the hyperparameter optimization process with more advanced optimization strategies than are available in other libraries.\n",
    "\n",
    "There are two ways to scale hyperopt with Apache Spark:\n",
    "* Use single-machine hyperopt with a distributed training algorithm (e.g. MLlib)\n",
    "* Use distributed hyperopt with single-machine training algorithms (e.g. scikit-learn) with the SparkTrials class. \n",
    "\n",
    "In this lesson, we will use single-machine hyperopt with MLlib, but in the lab, you will see how to use hyperopt to distribute the hyperparameter tuning of single node models. \n",
    "\n",
    "Unfortunately, you canâ€™t use hyperopt to distribute the hyperparameter optimization for distributed training algorithims at this time. However, you do still get the benefit of using more advanced hyperparameter search algorthims (random search, TPE, etc.) with Spark ML.\n",
    "\n",
    "\n",
    "Resources:\n",
    "\n",
    "0. <a href=\"http://hyperopt.github.io/hyperopt/scaleout/spark/\" target=\"_blank\">Documentation</a>\n",
    "0. <a href=\"https://docs.databricks.com/applications/machine-learning/automl/hyperopt/index.html\" target=\"_blank\">Hyperopt on Databricks</a>\n",
    "0. <a href=\"https://databricks.com/blog/2019/06/07/hyperparameter-tuning-with-mlflow-apache-spark-mllib-and-hyperopt.html\" target=\"_blank\">Hyperparameter Tuning with MLflow, Apache Spark MLlib and Hyperopt</a>\n",
    "0. <a href=\"https://databricks.com/blog/2021/04/15/how-not-to-tune-your-model-with-hyperopt.html\" target=\"_blank\">How (Not) to Tune Your Model With Hyperopt</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a8c3732-cf1a-44a2-be8c-3021f4384b24",
     "showTitle": false,
     "title": "--i18n-2340cdf4-9753-41b4-a613-043b90f0f472"
    }
   },
   "source": [
    "\n",
    "## Load Dataset\n",
    "\n",
    "Let's start by loading in our SF Airbnb Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01e23c33-844f-45fb-82a2-dffb621bdb30",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_path = f\"dbfs:/mnt/dbacademy-datasets/scalable-machine-learning-with-apache-spark/v02/airbnb/sf-listings/sf-listings-2019-03-06-clean.delta/\"\n",
    "airbnb_df = spark.read.format(\"delta\").load(file_path)\n",
    "# no longer just splitting between train and test data\n",
    "# adding validation data set to validate the performance of the model with different hyperparameters\n",
    "train_df, val_df, test_df = airbnb_df.randomSplit([.6, .2, .2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "774e1ae6-7761-4b47-b893-c7113a020e8c",
     "showTitle": false,
     "title": "--i18n-37bbd5bd-f330-4d02-8af6-1b185612cdf8"
    }
   },
   "source": [
    "\n",
    "## Build a Model Pipeline\n",
    "\n",
    "We will then create our random forest pipeline and regression evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72f3e51d-4c9a-4c69-a33b-b26f0741d4a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# \n",
    "categorical_cols = [field for (field, dataType) in train_df.dtypes if dataType == \"string\"]\n",
    "index_output_cols = [x + \"Index\" for x in categorical_cols]\n",
    "\n",
    "string_indexer = StringIndexer(inputCols=categorical_cols, outputCols=index_output_cols, handleInvalid=\"skip\")\n",
    "\n",
    "numeric_cols = [field for (field, dataType) in train_df.dtypes if ((dataType == \"double\") & (field != \"price\"))]\n",
    "assembler_inputs = index_output_cols + numeric_cols\n",
    "# vector assembler moves all the features into a single column as a list\n",
    "vec_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
    "\n",
    "# using random forest to predict the price \n",
    "rf = RandomForestRegressor(labelCol=\"price\", maxBins=40, seed=42)\n",
    "pipeline = Pipeline(stages=[string_indexer, vec_assembler, rf])\n",
    "# evaluating using the default RMSE metric\n",
    "regression_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3eb53ee7-152e-4271-bc99-29c4be2cafa9",
     "showTitle": false,
     "title": "--i18n-e4627900-f2a5-4f65-881e-1374187dd4f9"
    }
   },
   "source": [
    "\n",
    "## Define *Objective Function*\n",
    "\n",
    "Next, we get to the hyperopt-specific part of the workflow.\n",
    "\n",
    "First, we define our **objective function**. The objective function has two primary requirements:\n",
    "\n",
    "1. An **input** **`params`** including hyperparameter values to use when training the model\n",
    "2. An **output** containing a loss metric on which to optimize\n",
    "\n",
    "In this case, we are specifying values of **`max_depth`** and **`num_trees`** and returning the RMSE as our loss metric.\n",
    "\n",
    "We are reconstructing our pipeline for the **`RandomForestRegressor`** to use the specified hyperparameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd721e3c-a445-4667-b2e2-1d33da38f8b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def objective_function(params):    \n",
    "    # set the hyperparameters that we want to tune using \n",
    "    max_depth = params[\"max_depth\"]\n",
    "    num_trees = params[\"num_trees\"]\n",
    "\n",
    "    # this is just training the model not using hyperopt\n",
    "    with mlflow.start_run(): # track model builds \n",
    "        estimator = pipeline.copy({rf.maxDepth: max_depth, rf.numTrees: num_trees}) # copy our pipeline with the hyperparameter values set \n",
    "        model = estimator.fit(train_df) # make the model\n",
    "\n",
    "        preds = model.transform(val_df) # make predictions\n",
    "        rmse = regression_evaluator.evaluate(preds) # evaluate perfomance\n",
    "        mlflow.log_metric(\"rmse\", rmse) # log performance\n",
    "\n",
    "    return rmse # number hyperopt is going to try to minimize "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee9b23a0-fa98-4334-a028-d8c66ff4292e",
     "showTitle": false,
     "title": "--i18n-d4f9dd2b-060b-4eef-8164-442b2be242f4"
    }
   },
   "source": [
    "\n",
    "## Define *Search Space*\n",
    "\n",
    "Next, we define our search space. \n",
    "\n",
    "This is similar to the parameter grid in a grid search process. However, we are only specifying the range of values rather than the individual, specific values to be tested. It's up to hyperopt's optimization algorithm to choose the actual values.\n",
    "\n",
    "See the <a href=\"https://github.com/hyperopt/hyperopt/wiki/FMin\" target=\"_blank\">documentation</a> for helpful tips on defining your search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c68be47-7fcc-403e-bf99-810c82a2526e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "\n",
    "search_space = {\n",
    "    \"max_depth\": hp.quniform(\"max_depth\", 2, 5, 1), # label, min value, max value, increment value = integers only = 2, 3, 4, 5\n",
    "    \"num_trees\": hp.quniform(\"num_trees\", 10, 100, 1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15987dc7-6348-4975-8faa-24dd6e3e9a2c",
     "showTitle": false,
     "title": "--i18n-27891521-e481-4734-b21c-b2c5fe1f01fe"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "**`fmin()`** generates new hyperparameter configurations to use for your **`objective_function`**. It will evaluate 4 models in total, using the information from the previous models to make a more informative decision for the next hyperparameter to try. \n",
    "\n",
    "Hyperopt allows for parallel hyperparameter tuning using either random search or Tree of Parzen Estimators (TPE). Note that in the cell below, we are importing **`tpe`**. According to the <a href=\"http://hyperopt.github.io/hyperopt/scaleout/spark/\" target=\"_blank\">documentation</a>, TPE is an adaptive algorithm that \n",
    "\n",
    "> iteratively explores the hyperparameter space. Each new hyperparameter setting tested will be chosen based on previous results. \n",
    "\n",
    "Hence, **`tpe.suggest`** is a Bayesian method.\n",
    "\n",
    "MLflow also integrates with Hyperopt, so you can track the results of all the models youâ€™ve trained and their results as part of your hyperparameter tuning. Notice you can track the MLflow experiment in this notebook, but you can also specify an external experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0da4d6a-25f8-4993-94da-cb68432eb36c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/4 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/02/29 23:02:00 WARNING mlflow.utils: Truncated the value of the key `VectorAssembler.inputCols`. Truncated value: `['host_is_superhostIndex', 'cancellation_policyIndex', 'instant_bookableIndex', 'neighbourhood_cleansedIndex', 'property_typeIndex', 'room_typeIndex', 'bed_typeIndex', 'host_total_listings_count', 'latitude', 'longitude', 'accommodates', 'bathrooms', 'bedrooms', 'beds', 'minimum_nights', 'number_of_reviews', 'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value', 'be...`\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r 25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:29<01:27, 29.05s/trial, best loss: 357.82375290612674]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/02/29 23:02:29 WARNING mlflow.utils: Truncated the value of the key `VectorAssembler.inputCols`. Truncated value: `['host_is_superhostIndex', 'cancellation_policyIndex', 'instant_bookableIndex', 'neighbourhood_cleansedIndex', 'property_typeIndex', 'room_typeIndex', 'bed_typeIndex', 'host_total_listings_count', 'latitude', 'longitude', 'accommodates', 'bathrooms', 'bedrooms', 'beds', 'minimum_nights', 'number_of_reviews', 'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value', 'be...`\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:39<00:35, 17.96s/trial, best loss: 357.82375290612674]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/02/29 23:02:39 WARNING mlflow.utils: Truncated the value of the key `VectorAssembler.inputCols`. Truncated value: `['host_is_superhostIndex', 'cancellation_policyIndex', 'instant_bookableIndex', 'neighbourhood_cleansedIndex', 'property_typeIndex', 'room_typeIndex', 'bed_typeIndex', 'host_total_listings_count', 'latitude', 'longitude', 'accommodates', 'bathrooms', 'bedrooms', 'beds', 'minimum_nights', 'number_of_reviews', 'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value', 'be...`\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:46<00:13, 13.17s/trial, best loss: 357.82375290612674]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/02/29 23:02:46 WARNING mlflow.utils: Truncated the value of the key `VectorAssembler.inputCols`. Truncated value: `['host_is_superhostIndex', 'cancellation_policyIndex', 'instant_bookableIndex', 'neighbourhood_cleansedIndex', 'property_typeIndex', 'room_typeIndex', 'bed_typeIndex', 'host_total_listings_count', 'latitude', 'longitude', 'accommodates', 'bathrooms', 'bedrooms', 'beds', 'minimum_nights', 'number_of_reviews', 'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value', 'be...`\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:54<00:00, 10.96s/trial, best loss: 357.82375290612674]\r100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:54<00:00, 13.57s/trial, best loss: 357.82375290612674]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/02/29 23:02:54 WARNING mlflow.utils: Truncated the value of the key `VectorAssembler.inputCols`. Truncated value: `['host_is_superhostIndex', 'cancellation_policyIndex', 'instant_bookableIndex', 'neighbourhood_cleansedIndex', 'property_typeIndex', 'room_typeIndex', 'bed_typeIndex', 'host_total_listings_count', 'latitude', 'longitude', 'accommodates', 'bathrooms', 'bedrooms', 'beds', 'minimum_nights', 'number_of_reviews', 'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value', 'be...`\n2024/02/29 23:03:23 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n/databricks/python/lib/python3.9/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import fmin, tpe, Trials\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "mlflow.pyspark.ml.autolog(log_models=False)\n",
    "\n",
    "num_evals = 4\n",
    "trials = Trials()\n",
    "best_hyperparam = fmin(fn=objective_function, # training function = pipeline\n",
    "                       space=search_space, # search space = max depth and number of trees\n",
    "                       algo=tpe.suggest, # manner in which we search - grid search is just a brute force approach, whereas bayseian search is more adaptable\n",
    "                       max_evals=num_evals, # number of times it build numbers \n",
    "                       trials=trials, # where we store the results\n",
    "                       rstate=np.random.default_rng(42)) # likely equivalent to seed for consistency\n",
    "\n",
    "# With our best model only!\n",
    "# Retrain model on train & validation dataset and evaluate on test dataset \n",
    "with mlflow.start_run(): # use ML flow to track our model builds \n",
    "    best_max_depth = best_hyperparam[\"max_depth\"]\n",
    "    best_num_trees = best_hyperparam[\"num_trees\"]\n",
    "    estimator = pipeline.copy({rf.maxDepth: best_max_depth, rf.numTrees: best_num_trees})\n",
    "    combined_df = train_df.union(val_df) # Combine train & validation together\n",
    "\n",
    "    pipeline_model = estimator.fit(combined_df)\n",
    "    pred_df = pipeline_model.transform(test_df)\n",
    "    rmse = regression_evaluator.evaluate(pred_df)\n",
    "\n",
    "    # Log param and metrics for the final model\n",
    "    mlflow.log_param(\"maxDepth\", best_max_depth)\n",
    "    mlflow.log_param(\"numTrees\", best_num_trees)\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.spark.log_model(pipeline_model, \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "862ce478-5a3a-4a0b-99a9-85fa69aaa0c6",
     "showTitle": false,
     "title": "--i18n-a2c7fb12-fd0b-493f-be4f-793d0a61695b"
    }
   },
   "source": [
    "\n",
    "## Classroom Cleanup\n",
    "\n",
    "Run the following cell to remove lessons-specific assets created during this lesson:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd1d871d-5444-43cc-ae06-8fe597d4d1bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting the learning environment:\n| dropping the schema \"charlie_ohara_4mi2_da_sml\"...(1 seconds)\n| removing the working directory \"dbfs:/mnt/dbacademy-users/charlie.ohara@standard.ai/scalable-machine-learning-with-apache-spark\"...(0 seconds)\n\nValidating the locally installed datasets:\n| listing local files...(3 seconds)\n| validation completed...(3 seconds total)\n"
     ]
    }
   ],
   "source": [
    "DA.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "528c7da2-35fd-4cdc-a574-a24c4d290d2e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "&copy; 2023 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "ml_08_hyperopt",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
