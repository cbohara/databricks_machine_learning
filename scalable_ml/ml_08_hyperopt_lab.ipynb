{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b953a861-10b1-4d00-82e5-2118efcbd27c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f5907fd-ad02-44b8-b998-daf08a144291",
     "showTitle": false,
     "title": "--i18n-be8397b6-c087-4d7b-8302-5652eec27caf"
    }
   },
   "source": [
    "\n",
    "\n",
    " \n",
    "# Hyperopt Lab\n",
    "\n",
    "The <a href=\"https://github.com/hyperopt/hyperopt\" target=\"_blank\">Hyperopt library</a> allows for parallel hyperparameter tuning using either random search or Tree of Parzen Estimators (TPE). With MLflow, we can record the hyperparameters and corresponding metrics for each hyperparameter combination. You can read more on <a href=\"https://github.com/hyperopt/hyperopt/blob/master/docs/templates/scaleout/spark.md\" target=\"_blank\">SparkTrials w/ Hyperopt</a>.\n",
    "\n",
    "> SparkTrials fits and evaluates each model on one Spark executor, allowing massive scale-out for tuning. To use SparkTrials with Hyperopt, simply pass the SparkTrials object to Hyperopt's fmin() function.\n",
    "\n",
    "## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Learning Objectives:<br>\n",
    "\n",
    "By the end of this lab, you should be able to;\n",
    "\n",
    "* Train a single-node machine learning model in distributed way\n",
    "* Explain the difference between `SparkTrails` and default `Trails` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43f85fd5-a0e8-4f47-852a-f484eeba71b7",
     "showTitle": false,
     "title": "--i18n-1098dbd2-524d-498e-8f36-0da8245328a1"
    }
   },
   "source": [
    "## Lab Setup\n",
    "\n",
    "The first thing we're going to do is to **run setup script**. This script will define the required configuration variables that are scoped to each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8a043ac-5e76-4e50-adf3-8cff9f5f3f08",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nPython interpreter will be restarted.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting the learning environment:\n| No action taken\n\nSkipping install of existing datasets to \"dbfs:/mnt/dbacademy-datasets/scalable-machine-learning-with-apache-spark/v02\"\n\nValidating the locally installed datasets:\n| listing local files...(3 seconds)\n| validation completed...(3 seconds total)\n\nCreating & using the schema \"charlie_ohara_4mi2_da_sml\" in the catalog \"hive_metastore\"...(1 seconds)\n\nPredefined tables in \"charlie_ohara_4mi2_da_sml\":\n| -none-\n\nPredefined paths variables:\n| DA.paths.working_dir: dbfs:/mnt/dbacademy-users/charlie.ohara@standard.ai/scalable-machine-learning-with-apache-spark\n| DA.paths.user_db:     dbfs:/mnt/dbacademy-users/charlie.ohara@standard.ai/scalable-machine-learning-with-apache-spark/database.db\n| DA.paths.datasets:    dbfs:/mnt/dbacademy-datasets/scalable-machine-learning-with-apache-spark/v02\n\nSetup completed (9 seconds)\n"
     ]
    }
   ],
   "source": [
    "%run \"../Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e08426b-d347-4556-89e3-eef8808c1418",
     "showTitle": false,
     "title": "--i18n-13b0389c-cbd8-4b31-9f15-a6a9f18e8f60"
    }
   },
   "source": [
    "\n",
    "## Load Dataset\n",
    "\n",
    "Read in a cleaned version of the Airbnb dataset with just numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "864b1740-0a29-44f0-bbf1-8a7d328b8bcf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd # doing equivalent work on a single node vs spark parallelization\n",
    "\n",
    "df = pd.read_csv(f\"dbfs:/mnt/dbacademy-datasets/scalable-machine-learning-with-apache-spark/v02/airbnb/sf-listings/airbnb-cleaned-mlflow.csv\".replace(\"dbfs:/\", \"/dbfs/\")).drop([\"zipcode\"], axis=1)\n",
    "\n",
    "# split 80/20 train-test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop([\"price\"], axis=1),\n",
    "                                                    df[[\"price\"]].values.ravel(),\n",
    "                                                    test_size = 0.2,\n",
    "                                                    random_state = 42) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2d5aa87-3f73-4319-8fd5-1eb982dc7814",
     "showTitle": false,
     "title": "--i18n-b84062c7-9fb2-4d34-a196-98e5074c7ad4"
    }
   },
   "source": [
    "\n",
    "## Define Objective Function\n",
    "\n",
    "Now we need to define an **`objective_function`** where you evaluate the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\" target=\"_blank\">random forest's</a> predictions using R2.\n",
    "\n",
    "In the code below, compute the **`r2`** and return it (remember we are trying to maximize R2, so we need to return it as a negative value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd749e6c-7b14-4e31-9d20-54872b4c6542",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, r2_score\n",
    "from numpy import mean\n",
    "  \n",
    "def objective_function(params):\n",
    "    # set the hyperparameters that we want to tune:\n",
    "    max_depth = params['max_depth']\n",
    "    max_features = params['max_features']\n",
    "\n",
    "    regressor = RandomForestRegressor(max_depth=max_depth, max_features=max_features, random_state=42)\n",
    "\n",
    "    # Evaluate predictions\n",
    "    r2 = mean(cross_val_score(regressor, X_train, y_train, cv=3))\n",
    "\n",
    "    # Note: since we aim to maximize r2, we need to return it as a negative value (\"loss\": -metric)\n",
    "    return -r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5eb8f555-6c6e-4665-b44d-523b324d0f19",
     "showTitle": false,
     "title": "--i18n-7b10a96d-d868-4603-ab84-50388a8f50fc"
    }
   },
   "source": [
    "\n",
    "\n",
    "## Define Search Space\n",
    "\n",
    "We need to define a search space for HyperOpt. Let the **`max_depth`** vary between 2-10, and **`max_features`** be one of: \"auto\", \"sqrt\", or \"log2\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11f65cc9-edd4-4b6b-ba60-8009fdcf9d13",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "from hyperopt import hp\n",
    "\n",
    "# algos to use when choosing the max features to use for training\n",
    "max_features_choices =  [\"auto\", \"sqrt\", \"log2\"]\n",
    "search_space = {\n",
    "    \"max_depth\": hp.quniform(\"max_depth\", 2, 10, 1),\n",
    "    \"max_features\": hp.choice(\"max_features\", max_features_choices)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be1752f5-260d-4be1-b0f0-9d8fd69f656b",
     "showTitle": false,
     "title": "--i18n-6db6a36a-e1ca-400d-81fc-20ad5a794a01"
    }
   },
   "source": [
    "\n",
    "## Train Models Concurrently\n",
    "\n",
    "Instead of using the default **`Trials`** class, you can leverage the **`SparkTrials`** class to trigger the distribution of tuning tasks across Spark executors. On Databricks, SparkTrials are automatically logged with MLflow.\n",
    "\n",
    "**`SparkTrials`** takes 3 optional arguments, namely **`parallelism`**, **`timeout`**, and **`spark_session`**. You can refer to this <a href=\"http://hyperopt.github.io/hyperopt/scaleout/spark/\" target=\"_blank\">page</a> to read more.\n",
    "\n",
    "In the code below, fill in the **`fmin`** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "906c58fe-7379-452a-b1a8-68a6892e2c5d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperopt with SparkTrials will automatically track trials in MLflow. To view the MLflow experiment associated with the notebook, click the 'Runs' icon in the notebook context bar on the upper right. There, you can view all runs.\nTo view logs from trials, please check the Spark executor logs. To view executor logs, expand 'Spark Jobs' above until you see the (i) icon next to the stage from the trial job. Click it and find the list of tasks. Click the 'stderr' link for a task to view trial logs.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/8 [00:00<?, ?trial/s, best loss=?]\r 25%|██▌       | 2/8 [00:07<00:21,  3.59s/trial, best loss: -0.5718057846111207]\r 50%|█████     | 4/8 [00:12<00:11,  2.99s/trial, best loss: -0.6468700448781513]\r 75%|███████▌  | 6/8 [00:17<00:05,  2.80s/trial, best loss: -0.6658971922113791]\r 88%|████████▊ | 7/8 [00:22<00:03,  3.34s/trial, best loss: -0.6658971922113791]\r100%|██████████| 8/8 [00:23<00:00,  2.73s/trial, best loss: -0.6658971922113791]\r100%|██████████| 8/8 [00:23<00:00,  2.93s/trial, best loss: -0.6658971922113791]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Trials: 8: 8 succeeded, 0 failed, 0 cancelled.\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "from hyperopt import fmin, tpe, SparkTrials\n",
    "import mlflow\n",
    "import numpy as np\n",
    "\n",
    "# Number of models to evaluate\n",
    "num_evals = 8\n",
    "# Number of models to train concurrently\n",
    "spark_trials = SparkTrials(parallelism=2)\n",
    "# Automatically logs to MLflow\n",
    "best_hyperparam = fmin(\n",
    "    fn=objective_function, # what are we trying to maximize as an evluation metric?\n",
    "    space=search_space, # how we are going to search = using max_depth and max_features hyperparameters \n",
    "    max_evals=num_evals, # max number of models to train\n",
    "    trials=spark_trials, # where we are going to store the trials\n",
    "    algo=tpe.suggest # bayseian approach\n",
    ")\n",
    "\n",
    "# Re-train best model and log metrics on test dataset\n",
    "with mlflow.start_run(run_name=\"best_model\"):\n",
    "    # get optimal hyperparameter values\n",
    "    best_max_depth = best_hyperparam[\"max_depth\"]\n",
    "    best_max_features = max_features_choices[best_hyperparam[\"max_features\"]] # best hyperparam provides index for the max_features_choices list \n",
    "\n",
    "    # train model on entire training data\n",
    "    regressor = RandomForestRegressor(max_depth=best_max_depth, max_features=best_max_features, random_state=42)\n",
    "    regressor.fit(X_train, y_train)\n",
    "\n",
    "    # evaluate on holdout/test data\n",
    "    r2 = regressor.score(X_test, y_test)\n",
    "\n",
    "    # Log param and metric for the final model\n",
    "    mlflow.log_param(\"max_depth\", best_max_depth)\n",
    "    mlflow.log_param(\"max_features\", best_max_features)\n",
    "    mlflow.log_metric(\"loss\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c5ee8b0-1461-42da-8f89-66a1c8cab246",
     "showTitle": false,
     "title": "--i18n-398681fb-0ab4-4886-bb08-58117da3b7af"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "Now you can compare all of the models using the MLflow UI. \n",
    "\n",
    "To understand the effect of tuning a hyperparameter:\n",
    "\n",
    "0. Select the resulting runs and click Compare.\n",
    "0. In the Scatter Plot, select a hyperparameter for the X-axis and loss for the Y-axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d5931a7-1305-4267-8a93-83e0a08f2e9a",
     "showTitle": false,
     "title": "--i18n-a2c7fb12-fd0b-493f-be4f-793d0a61695b"
    }
   },
   "source": [
    "\n",
    "## Classroom Cleanup\n",
    "\n",
    "Run the following cell to remove lessons-specific assets created during this lesson:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52760058-95c1-457f-aebb-9a81defbc2a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DA.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff23e1ad-4376-4e80-a606-720fa191b519",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "&copy; 2023 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "ml_08_hyperopt_lab",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
