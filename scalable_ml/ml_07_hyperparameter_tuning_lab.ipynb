{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "277116eb-40e6-4b23-81da-9336e971b5b2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a24439fc-20d5-4bae-8ff9-61697f2e09e4",
     "showTitle": false,
     "title": "--i18n-b778c8d0-84e6-4192-a921-b9b60fd20d9b"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "# Hyperparameter Tuning with Random Forests\n",
    "\n",
    "In this lab, you will convert the Airbnb problem to a classification dataset, build a random forest classifier, and tune some hyperparameters of the random forest.\n",
    "\n",
    "## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Learning Objectives:<br>\n",
    "\n",
    "By the end of this lab, you should be able to;\n",
    "\n",
    "* Perform grid search on a random forest based model\n",
    "* Generate feature importance scores and classification metrics for a random forest model\n",
    "* Identify differences between scikit-learn's and Spark ML's Random Forest implementation\n",
    "\n",
    "\n",
    " \n",
    "You can read more about the distributed implementation of Random Forests in the Spark <a href=\"https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/tree/impl/RandomForest.scala#L42\" target=\"_blank\">source code</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a87fdd5-3a41-4da2-83f8-7941c08b1831",
     "showTitle": false,
     "title": "--i18n-40dbd041-1a78-4f06-b83f-e9ad3d51d6ed"
    }
   },
   "source": [
    "## Lab Setup\n",
    "\n",
    "The first thing we're going to do is to **run setup script**. This script will define the required configuration variables that are scoped to each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46378fb7-9947-408c-92a4-d471f6a0d13f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nPython interpreter will be restarted.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting the learning environment:\n| No action taken\n\nSkipping install of existing datasets to \"dbfs:/mnt/dbacademy-datasets/scalable-machine-learning-with-apache-spark/v02\"\n\nValidating the locally installed datasets:\n| listing local files...(4 seconds)\n| validation completed...(4 seconds total)\n\nCreating & using the schema \"charlie_ohara_4mi2_da_sml\" in the catalog \"hive_metastore\"...(0 seconds)\n\nPredefined tables in \"charlie_ohara_4mi2_da_sml\":\n| -none-\n\nPredefined paths variables:\n| DA.paths.working_dir: dbfs:/mnt/dbacademy-users/charlie.ohara@standard.ai/scalable-machine-learning-with-apache-spark\n| DA.paths.user_db:     dbfs:/mnt/dbacademy-users/charlie.ohara@standard.ai/scalable-machine-learning-with-apache-spark/database.db\n| DA.paths.datasets:    dbfs:/mnt/dbacademy-datasets/scalable-machine-learning-with-apache-spark/v02\n\nSetup completed (7 seconds)\n"
     ]
    }
   ],
   "source": [
    "%run \"../Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a7ee609-9ab5-4159-a707-92b0066a533f",
     "showTitle": false,
     "title": "--i18n-02dc0920-88e1-4f5b-886c-62b8cc02d1bb"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "## From Regression to Classification\n",
    "\n",
    "In this case, we'll turn the Airbnb housing dataset into a classification problem to **classify between high and low price listings.**  Our **`class`** column will be:<br><br>\n",
    "\n",
    "- **`0`** for a low cost listing of under $150\n",
    "- **`1`** for a high cost listing of $150 or more\n",
    "\n",
    "[random forest classifier](https://spark.apache.org/docs/latest/ml-classification-regression.html#random-forest-classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a5a9cd4-f2ad-44d7-8dcb-f071f9d85273",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "file_path = f\"dbfs:/mnt/dbacademy-datasets/scalable-machine-learning-with-apache-spark/v02/airbnb/sf-listings/sf-listings-2019-03-06-clean.delta/\"\n",
    "\n",
    "# add new column price class 0 = low cost, 1 = high cost \n",
    "airbnb_df = (spark\n",
    "            .read\n",
    "            .format(\"delta\")\n",
    "            .load(file_path)\n",
    "            # new target\n",
    "            .withColumn(\"priceClass\", (col(\"price\") >= 150).cast(\"int\"))\n",
    "            # drop previously used target \n",
    "            .drop(\"price\")\n",
    "           )\n",
    "\n",
    "train_df, test_df = airbnb_df.randomSplit([.8, .2], seed=42)\n",
    "\n",
    "# index categorical columns\n",
    "categorical_cols = [field for (field, dataType) in train_df.dtypes if dataType == \"string\"]\n",
    "index_output_cols = [x + \"Index\" for x in categorical_cols]\n",
    "string_indexer = StringIndexer(inputCols=categorical_cols, outputCols=index_output_cols, handleInvalid=\"skip\")\n",
    "\n",
    "numeric_cols = [field for (field, dataType) in train_df.dtypes if ((dataType == \"double\") & (field != \"priceClass\"))]\n",
    "\n",
    "assembler_inputs = index_output_cols + numeric_cols\n",
    "\n",
    "vec_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ecf75d5-cf7c-4aaf-bd1b-8c38b421ec49",
     "showTitle": false,
     "title": "--i18n-e3bb8033-43ea-439c-a134-36bedbeff408"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "### Why can't we OHE?\n",
    "\n",
    "**Question:** What would go wrong if we One Hot Encoded our variables before passing them into the random forest?\n",
    "\n",
    "**HINT:** Think about what would happen to the \"randomness\" of feature selection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a40e97d-260c-4716-a474-e2d605c9aae7",
     "showTitle": false,
     "title": "--i18n-0e9bdc2f-0d8d-41cb-9509-47833d66bc5e"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "## Random Forest\n",
    "\n",
    "Create a Random Forest classifer called **`rf`** with the **`labelCol=priceClass`**, **`maxBins=40`**, and **`seed=42`** (for reproducibility).\n",
    "\n",
    "It's under **`pyspark.ml.classification.RandomForestClassifier`** in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e0cffdb-bc81-421a-88af-23c9c46d0c2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# https://spark.apache.org/docs/latest/ml-classification-regression.html#random-forest-classifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"priceClass\",\n",
    "                            maxBins = 40,\n",
    "                            seed = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "456a75f0-ecae-42d4-a021-29e33e42341f",
     "showTitle": false,
     "title": "--i18n-7f3962e7-51b8-4477-9599-2465ab94a049"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "## Grid Search\n",
    "\n",
    "There are a lot of hyperparameters we could tune, and it would take a long time to manually configure.\n",
    "\n",
    "Let's use Spark's <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.ParamGridBuilder.html?highlight=paramgrid#pyspark.ml.tuning.ParamGridBuilder\" target=\"_blank\">ParamGridBuilder</a> to find the optimal hyperparameters in a more systematic approach. Call this variable **`param_grid`**.\n",
    "\n",
    "Let's define a grid of hyperparameters to test:\n",
    "  - maxDepth: max depth of the decision tree (Use the values **`2, 5, 10`**)\n",
    "  - numTrees: number of decision trees (Use the values **`10, 20, 100`**)\n",
    "\n",
    "**`addGrid()`** accepts the name of the parameter (e.g. **`rf.maxDepth`**), and a list of the possible values (e.g. **`[2, 5, 10]`**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5062ec80-baf9-4e89-8b92-cd8847a094e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# https://spark.apache.org/docs/latest/ml-tuning.html\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# create 9 different models with all the combos of hyper parameters \n",
    "grid = ParamGridBuilder().addGrid(rf.maxDepth, [2, 5, 10]).addGrid(rf.numTrees, [10, 20, 100]).build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c16e47a-c989-45c6-af4e-8f59c3e68527",
     "showTitle": false,
     "title": "--i18n-e1862bae-e31e-4f5a-ab0e-926261c4e27b"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "## Evaluator\n",
    "\n",
    "In the past, we used a **`RegressionEvaluator`**.  For classification, we can use a <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.evaluation.BinaryClassificationEvaluator.html?highlight=binaryclass#pyspark.ml.evaluation.BinaryClassificationEvaluator\" target=\"_blank\">BinaryClassificationEvaluator</a> if we have two classes or <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.evaluation.MulticlassClassificationEvaluator.html?highlight=multiclass#pyspark.ml.evaluation.MulticlassClassificationEvaluator\" target=\"_blank\">MulticlassClassificationEvaluator</a> for more than two classes.\n",
    "\n",
    "Create a **`BinaryClassificationEvaluator`** with **`areaUnderROC`** as the metric.\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_24.png\"/> <a href=\"https://en.wikipedia.org/wiki/Receiver_operating_characteristic\" target=\"_blank\">Read more on ROC curves here.</a>  In essence, it compares true positive and false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "487c6f38-6398-458e-b323-48409ef23dcd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# use a different metric for classification vs regression\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"priceClass\", metricName=\"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65b2ac99-8ad3-4e11-a46f-a1e60798259e",
     "showTitle": false,
     "title": "--i18n-ea1c0e11-125d-4067-bd70-0bd6c7ca3cdb"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "## Cross Validation\n",
    "\n",
    "We are going to do 3-Fold cross-validation and set the **`seed`**=42 on the cross-validator for reproducibility.\n",
    "\n",
    "Put the Random Forest in the CV to speed up the <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.CrossValidator.html?highlight=crossvalidator#pyspark.ml.tuning.CrossValidator\" target=\"_blank\">cross validation</a> (as opposed to the pipeline in the CV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "124d142e-120a-4ba3-946c-189262db74f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "cv = CrossValidator(estimator=rf, evaluator=evaluator, estimatorParamMaps=grid, seed=42, numFolds=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d50fd2d-e9c6-4aa4-b48b-99823e81b95a",
     "showTitle": false,
     "title": "--i18n-1f8cebd5-673c-4513-b73b-b64b0a56297c"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "## Pipeline\n",
    "\n",
    "Let's fit the pipeline with our cross validator to our training data (this may take a few minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b265b091-0dd3-4f0a-b1ef-b972095a4bd0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stages = [string_indexer, vec_assembler, cv]\n",
    "\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "# automatically save the best performing model\n",
    "# will likely take longer as the number of models created for comparison increases \n",
    "pipeline_model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bffd8f14-540e-4577-a7bb-9d5e809cba2f",
     "showTitle": false,
     "title": "--i18n-70cdbfa3-0dd7-4f23-b755-afc0dadd7eb2"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "## Hyperparameter\n",
    "\n",
    "Which hyperparameter combination performed the best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "446cccff-d3f1-4f81-b253-043af0c15cbb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({Param(parent='RandomForestClassifier_21f0b5a6d39f', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 2, Param(parent='RandomForestClassifier_21f0b5a6d39f', name='numTrees', doc='Number of trees to train (>= 1).'): 10}, 0.8494609892340327), ({Param(parent='RandomForestClassifier_21f0b5a6d39f', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 2, Param(parent='RandomForestClassifier_21f0b5a6d39f', name='numTrees', doc='Number of trees to train (>= 1).'): 20}, 0.8450403538026396), ({Param(parent='RandomForestClassifier_21f0b5a6d39f', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 2, Param(parent='RandomForestClassifier_21f0b5a6d39f', name='numTrees', doc='Number of trees to train (>= 1).'): 100}, 0.857236821089423), ({Param(parent='RandomForestClassifier_21f0b5a6d39f', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 5, Param(parent='RandomForestClassifier_21f0b5a6d39f', name='numTrees', doc='Number of trees to train (>= 1).'): 10}, 0.8794946343548474), ({Param(parent='RandomForestClassifier_21f0b5a6d39f', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 5, Param(parent='RandomForestClassifier_21f0b5a6d39f', name='numTrees', doc='Number of trees to train (>= 1).'): 20}, 0.8872232007414573), ({Param(parent='RandomForestClassifier_21f0b5a6d39f', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 5, Param(parent='RandomForestClassifier_21f0b5a6d39f', name='numTrees', doc='Number of trees to train (>= 1).'): 100}, 0.8882009830669132), ({Param(parent='RandomForestClassifier_21f0b5a6d39f', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 10, Param(parent='RandomForestClassifier_21f0b5a6d39f', name='numTrees', doc='Number of trees to train (>= 1).'): 10}, 0.9048881178918164), ({Param(parent='RandomForestClassifier_21f0b5a6d39f', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 10, Param(parent='RandomForestClassifier_21f0b5a6d39f', name='numTrees', doc='Number of trees to train (>= 1).'): 20}, 0.9142624641849966), ({Param(parent='RandomForestClassifier_21f0b5a6d39f', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 10, Param(parent='RandomForestClassifier_21f0b5a6d39f', name='numTrees', doc='Number of trees to train (>= 1).'): 100}, 0.9180625529465044)]\n"
     ]
    }
   ],
   "source": [
    "cv_model = pipeline_model.stages[-1\n",
    "\n",
    "# see all the combos\n",
    "results = list(zip(cv_model.getEstimatorParamMaps(), cv_model.avgMetrics))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb7085b8-b486-44bc-9eba-f242d5f2b78c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassificationModel: uid=RandomForestClassifier_21f0b5a6d39f, numTrees=100, numClasses=2, numFeatures=33\n"
     ]
    }
   ],
   "source": [
    "rf_model = cv_model.bestModel\n",
    "print(rf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "568403d7-87f4-4b36-899b-0523f69c1b82",
     "showTitle": false,
     "title": "--i18n-11e6c47a-ddb1-416d-92a5-2f61340f9a5e"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5a13d35-155d-46a8-a717-8bc1870c1b30",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bedrooms</td>\n",
       "      <td>0.156562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>room_typeIndex</td>\n",
       "      <td>0.145999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>accommodates</td>\n",
       "      <td>0.145594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neighbourhood_cleansedIndex</td>\n",
       "      <td>0.094439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>beds</td>\n",
       "      <td>0.074897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>host_total_listings_count</td>\n",
       "      <td>0.056620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>latitude</td>\n",
       "      <td>0.051551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>longitude</td>\n",
       "      <td>0.039338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>review_scores_rating</td>\n",
       "      <td>0.033169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>number_of_reviews</td>\n",
       "      <td>0.031997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>minimum_nights</td>\n",
       "      <td>0.029923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bathrooms</td>\n",
       "      <td>0.028360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>property_typeIndex</td>\n",
       "      <td>0.027096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cancellation_policyIndex</td>\n",
       "      <td>0.016465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>review_scores_cleanliness</td>\n",
       "      <td>0.013874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>review_scores_location</td>\n",
       "      <td>0.009767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>review_scores_accuracy</td>\n",
       "      <td>0.008065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>host_is_superhostIndex</td>\n",
       "      <td>0.007745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>instant_bookableIndex</td>\n",
       "      <td>0.006643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>review_scores_value</td>\n",
       "      <td>0.006014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>review_scores_communication</td>\n",
       "      <td>0.003827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>review_scores_checkin</td>\n",
       "      <td>0.002303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>review_scores_rating_na</td>\n",
       "      <td>0.001572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bed_typeIndex</td>\n",
       "      <td>0.001396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>review_scores_accuracy_na</td>\n",
       "      <td>0.001294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>review_scores_communication_na</td>\n",
       "      <td>0.001186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>review_scores_checkin_na</td>\n",
       "      <td>0.001121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>review_scores_value_na</td>\n",
       "      <td>0.001037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>review_scores_location_na</td>\n",
       "      <td>0.000928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>review_scores_cleanliness_na</td>\n",
       "      <td>0.000905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>bathrooms_na</td>\n",
       "      <td>0.000222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>bedrooms_na</td>\n",
       "      <td>0.000047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>beds_na</td>\n",
       "      <td>0.000045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature</th>\n      <th>importance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>12</th>\n      <td>bedrooms</td>\n      <td>0.156562</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>room_typeIndex</td>\n      <td>0.145999</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>accommodates</td>\n      <td>0.145594</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>neighbourhood_cleansedIndex</td>\n      <td>0.094439</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>beds</td>\n      <td>0.074897</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>host_total_listings_count</td>\n      <td>0.056620</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>latitude</td>\n      <td>0.051551</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>longitude</td>\n      <td>0.039338</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>review_scores_rating</td>\n      <td>0.033169</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>number_of_reviews</td>\n      <td>0.031997</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>minimum_nights</td>\n      <td>0.029923</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>bathrooms</td>\n      <td>0.028360</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>property_typeIndex</td>\n      <td>0.027096</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>cancellation_policyIndex</td>\n      <td>0.016465</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>review_scores_cleanliness</td>\n      <td>0.013874</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>review_scores_location</td>\n      <td>0.009767</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>review_scores_accuracy</td>\n      <td>0.008065</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>host_is_superhostIndex</td>\n      <td>0.007745</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>instant_bookableIndex</td>\n      <td>0.006643</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>review_scores_value</td>\n      <td>0.006014</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>review_scores_communication</td>\n      <td>0.003827</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>review_scores_checkin</td>\n      <td>0.002303</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>review_scores_rating_na</td>\n      <td>0.001572</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>bed_typeIndex</td>\n      <td>0.001396</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>review_scores_accuracy_na</td>\n      <td>0.001294</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>review_scores_communication_na</td>\n      <td>0.001186</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>review_scores_checkin_na</td>\n      <td>0.001121</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>review_scores_value_na</td>\n      <td>0.001037</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>review_scores_location_na</td>\n      <td>0.000928</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>review_scores_cleanliness_na</td>\n      <td>0.000905</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>bathrooms_na</td>\n      <td>0.000222</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>bedrooms_na</td>\n      <td>0.000047</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>beds_na</td>\n      <td>0.000045</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pandas_df = pd.DataFrame(list(zip(vec_assembler.getInputCols(), rf_model.featureImportances)), columns=[\"feature\", \"importance\"])\n",
    "top_features = pandas_df.sort_values([\"importance\"], ascending=False)\n",
    "top_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b2d23ad-eace-44f6-81f8-73bd21052fec",
     "showTitle": false,
     "title": "--i18n-ae7e312e-d32b-4b02-97ff-ad4d2c737892"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "Do those features make sense? Would you use those features when picking an Airbnb rental?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce373a57-d017-4647-9db6-7cbdfde97cd8",
     "showTitle": false,
     "title": "--i18n-950eb40f-b1d2-4e7f-8b07-76faff6b8186"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "## Apply Model to Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40f2a2ad-4ebd-4db7-924d-fe68f136333a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC is 0.92\n"
     ]
    }
   ],
   "source": [
    "# .92 is a great value - easier to predict a class vs a specific value\n",
    "pred_df = pipeline_model.transform(test_df)\n",
    "area_under_roc = evaluator.evaluate(pred_df)\n",
    "print(f\"Area under ROC is {area_under_roc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa058ecc-52f1-4652-b1e9-de9b55180a69",
     "showTitle": false,
     "title": "--i18n-01974668-f242-4b8a-ac80-adda3b98392d"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "## Save Model\n",
    "\n",
    "Save the model to **`DA.paths.working_dir`** (variable defined in Classroom-Setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ec3af8c-2d09-4476-ba28-abcc9392f3d4",
     "showTitle": false,
     "title": "--i18n-f5fdf1a9-2a65-4252-aa76-18807dbb3a9d"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "## Sklearn vs SparkML\n",
    "\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\" target=\"_blank\">Sklearn RandomForestRegressor</a> vs <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.RandomForestRegressor.html?highlight=randomfore#pyspark.ml.regression.RandomForestRegressor\" target=\"_blank\">SparkML RandomForestRegressor</a>.\n",
    "\n",
    "Look at these params in particular:\n",
    "* **n_estimators** (sklearn) vs **numTrees** (SparkML)\n",
    "* **max_depth** (sklearn) vs **maxDepth** (SparkML)\n",
    "* **max_features** (sklearn) vs **featureSubsetStrategy** (SparkML)\n",
    "* **maxBins** (SparkML only)\n",
    "\n",
    "What do you notice that is different?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3cb96afa-8ace-42b3-9e00-956dd5e2ba3c",
     "showTitle": false,
     "title": "--i18n-a2c7fb12-fd0b-493f-be4f-793d0a61695b"
    }
   },
   "source": [
    "\n",
    "## Classroom Cleanup\n",
    "\n",
    "Run the following cell to remove lessons-specific assets created during this lesson:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd57e62d-f5d8-4582-8a4d-3f6966b6e99d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting the learning environment:\n| dropping the schema \"charlie_ohara_4mi2_da_sml\"...(0 seconds)\n| removing the working directory \"dbfs:/mnt/dbacademy-users/charlie.ohara@standard.ai/scalable-machine-learning-with-apache-spark\"...(0 seconds)\n\nValidating the locally installed datasets:\n| listing local files...(3 seconds)\n| validation completed...(3 seconds total)\n"
     ]
    }
   ],
   "source": [
    "DA.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90fd2142-00be-42a1-803a-c96fe7b87aa1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "&copy; 2023 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "ml_07_hyperparameter_tuning_lab",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
